{"cells":[{"cell_type":"markdown","metadata":{"id":"ky_aRwDU9Og5"},"source":["## Neural Machine Translation\n","\n","## Introduction\n","In this notebook, we will build a deep neural network that functions as part of an end-to-end machine translation pipeline. The completed pipeline will accept french text as input and return the english translation.\n","\n","__Proposed Approach__\n","\n","- Preprocessing: Load and examine data, cleaning, tokenization, padding\n","- Modeling: Build an encoder-decoder model with Attention\n","- Prediction: Generate specific translations of French to English, and compare the output translations to the ground truth translations"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9237,"status":"ok","timestamp":1706133216430,"user":{"displayName":"HAMZA BOUKTITIYA","userId":"05385177150877535559"},"user_tz":-60},"id":"piZrj6EHdlc9","outputId":"f21a7145-4993-4b67-90a6-044d8eb030dd"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (2.15.0)\n"]}],"source":["!pip install keras"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VbeBj9kK9Og-"},"outputs":[],"source":["from __future__ import print_function, division\n","from builtins import range, input\n","import pickle\n","import os, sys\n","import string\n","import pandas as pd\n","import re\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import matplotlib.pyplot as plt\n","\n","\n","from keras.models import Model\n","from keras.layers import Input, LSTM, GRU, Dense, Embedding, \\\n","  Bidirectional, RepeatVector, Concatenate, Activation, Dot, Lambda\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","import keras.backend as K\n","from keras.callbacks import ModelCheckpoint\n","from keras.models import load_model\n","#from keras.optimizers import adam\n","from keras import optimizers\n","from sklearn.model_selection import train_test_split\n","\n"]},{"cell_type":"markdown","metadata":{"id":"kgq0xegC9OhB"},"source":["### Model Configurations"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J68LJ-jF9OhC"},"outputs":[],"source":["# config\n","BATCH_SIZE = 512\n","EPOCHS = 100\n","LATENT_DIM = 256\n","LATENT_DIM_DECODER = 256\n","EMBEDDING_DIM = 200\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2165,"status":"ok","timestamp":1706139442240,"user":{"displayName":"HAMZA BOUKTITIYA","userId":"05385177150877535559"},"user_tz":-60},"id":"n8HEAjW5_v6U","outputId":"e98f1ea0-909b-4b8e-960d-77f97a83cbd3"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"ycDTZCec9OhC"},"source":["## Dataset\n","We begin by investigating the dataset that will be used to train and evaluate your pipeline.  \n","\n","### Load Data - Train and Test\n","The `data_en` file contains English sentences with their French translations in the `data_fr` file. Load the English and French data from these files from running the cell below."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1blZMQhm9OhD"},"outputs":[],"source":["with open('/content/drive/MyDrive/NLP/translation/French_test.pkl', 'rb') as handle:\n","    fr_test = pickle.load(handle)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OdH0Eopz9OhD"},"outputs":[],"source":["with open('/content/drive/MyDrive/NLP/translation/English_test.pkl', 'rb') as handle:\n","    en_test = pickle.load(handle)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0yj0fvDu9OhD"},"outputs":[],"source":["with open('/content/drive/MyDrive/NLP/translation/French_train.pkl', 'rb') as handle:\n","    fr_train = pickle.load(handle)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0AYEZXv89OhE"},"outputs":[],"source":["with open('/content/drive/MyDrive/NLP/translation/English_train.pkl', 'rb') as handle:\n","    en_train = pickle.load(handle)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gHFoaNIF9OhE"},"outputs":[],"source":["NUM_SAMPLES=len(fr_test)+len(fr_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17,"status":"ok","timestamp":1706139442773,"user":{"displayName":"HAMZA BOUKTITIYA","userId":"05385177150877535559"},"user_tz":-60},"id":"Mfk8KjEBiQRr","outputId":"7e504ad7-2ab1-4fdb-9ed0-60d2ba280129"},"outputs":[{"name":"stdout","output_type":"stream","text":["                                             english  \\\n","0                They're hiding something, I'm sure!   \n","1    It's obvious they're trying to keep their cool.   \n","2            the hotels don't seem very comfortable.   \n","3  he is probably about to be laid off by head of...   \n","4                         of course he's depressive!   \n","\n","                                   darija  \n","0    homa mkhbbyin chi haja, ana mti99en!  \n","1      bayna homa tay7awlo ib9aw mbrrdin.  \n","2  loTilat mabaynach fihom mori7in bzzaf.  \n","3      ghaliban ghayjrriw 3lih mn lkhdma!  \n","4                     Tab3an rah mkta2eb!  \n"]}],"source":["import pandas as pd\n","\n","# Replace 'path_to_your_dataset.csv' with the actual path to your dataset file\n","dataset_path = '/content/drive/MyDrive/NLP/translation/sentences.csv'\n","\n","# Read the dataset into a pandas DataFrame\n","# You may need to adjust parameters like 'sep' depending on the format of your file\n","df = pd.read_csv(dataset_path, sep=',')\n","df.columns = ['english', 'darija']\n","\n","# Example: Printing the first few entries\n","print(df.head())\n","\n","# Now, extract the English and Darija sentences into lists\n","english_sentences = df['english'].tolist()\n","darija_sentences = df['darija'].tolist()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Cfm44YMciimy"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13,"status":"ok","timestamp":1706139442774,"user":{"displayName":"HAMZA BOUKTITIYA","userId":"05385177150877535559"},"user_tz":-60},"id":"6K1c1Rif9OhE","outputId":"1eb1b85a-1686-4792-9ba2-d2e3d55bec59"},"outputs":[{"name":"stdout","output_type":"stream","text":["Sample train size: 10001\n"]}],"source":["input_texts = [] # Darija sentences\n","target_texts = [] # English sentences\n","target_texts_inputs = [] # English sentences offset by 1 for teacher forcing\n","\n","# Converting to lowercase\n","darija_train=[line.lower() for line in darija_sentences] # Replace fr_train with darija_sentences\n","en_train=[line.lower() for line in english_sentences] # Replace en_train with english_sentences\n","\n","NUM_SAMPLES=len(darija_train) # Replace fr_train with darija_train\n","print(\"Sample train size:\",NUM_SAMPLES)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1706139442774,"user":{"displayName":"HAMZA BOUKTITIYA","userId":"05385177150877535559"},"user_tz":-60},"id":"mIepAXQT9OhF","outputId":"da5c7889-1829-47e9-b278-f56d4f03a20e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Train set size (Darija): 4000\n","Train set size (English): 4000\n","Test set size (Darija): 6001\n","Test set size (English): 6001\n"]}],"source":["from sklearn.model_selection import train_test_split\n","\n","# Split the data into training and test sets\n","darija_train, darija_test, english_train, english_test = train_test_split(darija_train,en_train, test_size=0.6)\n","\n","print(\"Train set size (Darija):\", len(darija_train))\n","print(\"Train set size (English):\", len(english_train))\n","print(\"Test set size (Darija):\", len(darija_test))\n","print(\"Test set size (English):\", len(english_test))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"abtI5564hBbe"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"Xbf4Jr6g9OhF"},"source":["From looking at the sentences, you can see that they have been preprocessed already.  The puncuations have been delimited using spaces. All the text have been converted to lowercase.  This should save you some time, but the text requires more preprocessing."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e9JlLCF_hCBi"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"loYw3QqH9OhG"},"source":["## Preprocess\n","For this project, you won't use text data as input to your model. Instead, you'll convert the text into sequences of integers using the following preprocess methods:\n","1. Add start and end tokens to the sentences\n","2. Tokenize the words of the sentences\n","3. Add padding to make all the sequences of same length.\n","\n","Time to start preprocessing the data...\n","### Tokenize\n","For a neural network to predict on text data, it first has to be turned into data it can understand. Text data like \"dog\" is a sequence of ASCII character encodings.  Since a neural network is a series of multiplication and addition operations, the input data needs to be number(s).\n","\n","We can turn each character into a number or each word into a number.  These are called character and word ids, respectively.  Character ids are used for character level models that generate text predictions for each character.  A word level model uses word ids that generate text predictions for each word.  Word level models tend to learn better, since they are lower in complexity, so we'll use those.\n","\n","Turn each sentence into a sequence of words ids using Keras's [`Tokenizer`](https://keras.io/preprocessing/text/#tokenizer) function. Use this function to tokenize `english_sentences` and `french_sentences` in the cell below.\n","\n","\n","### Padding\n","When batching the sequence of word ids together, each sequence needs to be the same length. Since sentences are dynamic in length, we can add padding to the end of the sequences to make them the same length.\n","\n","Make sure all the English sequences have the same length and all the French sequences have the same length by adding padding to the end of each sequence using Keras's [`pad_sequences`](https://keras.io/preprocessing/sequence/#pad_sequences) function."]},{"cell_type":"markdown","metadata":{"id":"HiHTJWs99OhG"},"source":["__Preprocess Train Data__"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iSM1CD359OhG"},"outputs":[],"source":["#Adding \u003csos\u003e and \u003ceos\u003e tokens\n","\n","\n","\n","for lines in english_train:\n","    target_texts_inputs.append('\u003csos\u003e'+\" \"+ lines)\n","\n","for lines in english_train:\n","    target_texts.append(lines+ \" \" +'\u003ceos\u003e')\n","\n","for lines in darija_train:\n","    input_texts.append(lines)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":336,"status":"ok","timestamp":1706139443104,"user":{"displayName":"HAMZA BOUKTITIYA","userId":"05385177150877535559"},"user_tz":-60},"id":"8eXxtVcd9OhG","outputId":"1584e4cd-ab92-4bc9-d0f7-0728c887e5ba"},"outputs":[{"name":"stdout","output_type":"stream","text":["Data Preprocessed\n","Max train English sentence length: 28\n","Max train Darija sentence length: 22\n","Unique Darija Vocabulary count: 6485\n","Unique English Vocabulary count: 4051\n"]}],"source":["# tokenize Darija sentences\n","tokenizer_inputs = Tokenizer()\n","tokenizer_inputs.fit_on_texts(input_texts)\n","input_sequences = tokenizer_inputs.texts_to_sequences(input_texts)\n","\n","# word to index mapping for Darija\n","word2idx_inputs = tokenizer_inputs.word_index\n","\n","# Max length of Darija sentence\n","max_len_input = max(len(s) for s in input_sequences)\n","\n","# tokenize English sentences\n","tokenizer_outputs = Tokenizer(filters='')\n","tokenizer_outputs.fit_on_texts(target_texts + target_texts_inputs)\n","target_sequences = tokenizer_outputs.texts_to_sequences(target_texts)\n","target_sequences_inputs = tokenizer_outputs.texts_to_sequences(target_texts_inputs)\n","\n","# Word to index mapping for English\n","word2idx_outputs = tokenizer_outputs.word_index\n","\n","# store number of output words for later\n","num_words_output = len(word2idx_outputs) + 1\n","\n","# Max length of English sentence\n","max_len_target = max(len(s) for s in target_sequences)\n","\n","# pad the sequences\n","encoder_inputs = pad_sequences(input_sequences, maxlen=max_len_input)\n","decoder_inputs = pad_sequences(target_sequences_inputs, maxlen=max_len_target, padding='post')\n","decoder_targets = pad_sequences(target_sequences, maxlen=max_len_target, padding='post')\n","\n","print('Data Preprocessed')\n","print(\"Max train English sentence length:\", max_len_target)\n","print(\"Max train Darija sentence length:\", max_len_input)  # Change reference from French to Darija\n","print(\"Unique Darija Vocabulary count:\",len(word2idx_inputs))  # Change reference from French to Darija\n","print(\"Unique English Vocabulary count:\",len(word2idx_outputs))"]},{"cell_type":"markdown","metadata":{"id":"vK_pE4rz9OhH"},"source":["__Preprocess Test Data__"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VZba4VJg9OhH"},"outputs":[],"source":["##Test Preparation\n","input_texts_test = [] # Darija sentences\n","target_texts_test = [] # English sentences\n","target_texts_inputs_test = [] # English sentences offset by 1 for teacher forcing\n","\n","#Converting to lowercase\n","darija_test = [line.lower() for line in darija_test] # Changed from fr_test to darija_test\n","english_test = [line.lower() for line in english_test] # Changed from en_test to english_test"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"av1vZcBI9OhH"},"outputs":[],"source":["#Adding \u003csos\u003e and \u003ceos\u003e tokens\n","\n","for lines in english_test:\n","    target_texts_inputs_test.append('\u003csos\u003e'+\" \"+ lines)\n","\n","for lines in english_test:\n","    target_texts_test.append(lines+ \" \" +'\u003ceos\u003e')\n","\n","for lines in darija_test:\n","    input_texts_test.append(lines)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1706139444770,"user":{"displayName":"HAMZA BOUKTITIYA","userId":"05385177150877535559"},"user_tz":-60},"id":"6crk6a3z9OhH","outputId":"fe743f30-747b-4b60-9823-5875561d1fcf"},"outputs":[{"name":"stdout","output_type":"stream","text":["Max test English sentence length: 28\n","Max test Darija sentence length: 22\n"]}],"source":["# tokenize Darija sentences\n","\n","input_sequences_test = tokenizer_inputs.texts_to_sequences(input_texts_test)\n","\n","\n","target_sequences_test = tokenizer_outputs.texts_to_sequences(target_texts_test)\n","target_sequences_inputs_test = tokenizer_outputs.texts_to_sequences(target_texts_inputs_test)\n","\n","# pad the sequences\n","encoder_inputs_test = pad_sequences(input_sequences_test, maxlen=max_len_input)\n","decoder_inputs_test = pad_sequences(target_sequences_inputs_test, maxlen=max_len_target, padding='post')\n","decoder_targets_test = pad_sequences(target_sequences_test, maxlen=max_len_target, padding='post')\n","\n","print(\"Max test English sentence length:\", decoder_inputs_test.shape[1])\n","print(\"Max test Darija sentence length:\", encoder_inputs_test.shape[1])  # Changed reference from French to Darija"]},{"cell_type":"markdown","metadata":{"id":"vTA97Fw59OhH"},"source":["### Embedding\n","\n","In word embeddings, every word is represented as an n-dimensional dense vector. The words that are similar will have similar vector. The position of a word within the vector space is learned from text and is based on the words that surround the word when it is used.\n","\n","For French sentences, i.e. the inputs, we will use the [`GloVe`](https://nlp.stanford.edu/projects/glove/) word embeddings. For the translated English sentences in the output, we will use custom word embeddings.\n","\n","Let's create word embeddings for the inputs first. To do so, we need to load the GloVe word vectors into memory. We will then create a dictionary where words are the keys and the corresponding vectors are values, as shown below:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":397,"status":"ok","timestamp":1706139446181,"user":{"displayName":"HAMZA BOUKTITIYA","userId":"05385177150877535559"},"user_tz":-60},"id":"h4WILpDsokNv","outputId":"6e5a2869-9918-44a6-cf0a-1f9277bd7ac3"},"outputs":[{"name":"stdout","output_type":"stream","text":["Found 6203 word vectors.\n"]}],"source":["import os\n","import pandas as pd\n","from sklearn.preprocessing import OneHotEncoder\n","\n","path = '/content/drive/MyDrive/NLP/translation/syntactic categories'\n","files = os.listdir(path)\n","\n","df = pd.DataFrame()\n","\n","# read all .csv files and assign category based on file name\n","for file in files:\n","    if file.endswith('.csv'):\n","        category = file[:-4]  # remove '.csv'\n","        df_temp = pd.read_csv(os.path.join(path, file), names=['word'])\n","        df_temp['category'] = category\n","        df = pd.concat([df, df_temp])\n","\n","# create one-hot encoding of the categories\n","encoder = OneHotEncoder()\n","encoded_categories = encoder.fit_transform(df[['category']]).toarray()\n","\n","# create dictionary mapping words to one-hot encoded categories\n","word2vec = dict(zip(df['word'], encoded_categories))\n","\n","print('Found %s word vectors.' % len(word2vec))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1706139446643,"user":{"displayName":"HAMZA BOUKTITIYA","userId":"05385177150877535559"},"user_tz":-60},"id":"S2YjqnEzqTia","outputId":"7894f5c5-1fa4-45f9-c944-ce5de7ab765b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Word: trfi8\n","Category: verb-to-noun\n","Vector: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n","\n","Word: proceed\n","Category: verbs\n","Vector: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n","\n","Word: beside\n","Category: prepositions\n","Vector: [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n","\n","Word: theirs\n","Category: pronouns\n","Vector: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n","\n","Word: l3ach9a\n","Category: (in)definite\n","Vector: [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","\n","Word: respectable\n","Category: adjectives\n","Vector: [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","\n","Word: mchchat\n","Category: masculine_feminine_plural\n","Vector: [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n","\n","Word: conscience\n","Category: nouns\n","Vector: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n","\n","Word: 3rrfo\n","Category: imperatives\n","Vector: [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n","\n","Word: kay8enn2o\n","Category: conjug_present\n","Vector: [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n","\n","Word: nearby\n","Category: adverbs\n","Vector: [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","\n","Word: ttaSlo\n","Category: conjug_past\n","Vector: [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n","\n"]}],"source":["import random\n","\n","# Get unique categories\n","categories = df['category'].unique()\n","\n","# Empty dict to store a random word from each category\n","random_words = {}\n","\n","# Get a random word from each category\n","for category in categories:\n","    words_in_category = df[df['category'] == category]['word']\n","    random_word = random.choice(words_in_category)\n","    random_words[random_word] = {\"vector\": word2vec[random_word], \"category\": category}\n","\n","# Print the random words and their vectors\n","for word, info in random_words.items():\n","    print(\"Word:\", word)\n","    print(\"Category:\", info[\"category\"])\n","    print(\"Vector:\", info[\"vector\"])\n","    print()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1706139447348,"user":{"displayName":"HAMZA BOUKTITIYA","userId":"05385177150877535559"},"user_tz":-60},"id":"7EaRvkn0qz-n","outputId":"c172ab55-247b-49b4-95c0-ba9a30fda4e1"},"outputs":[{"name":"stdout","output_type":"stream","text":["Categories: [array(['(in)definite', 'adjectives', 'adverbs', 'conjug_past',\n","       'conjug_present', 'imperatives', 'masculine_feminine_plural',\n","       'nouns', 'prepositions', 'pronouns', 'verb-to-noun', 'verbs'],\n","      dtype=object)]\n"]}],"source":["print(\"Categories:\", encoder.categories_)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7a-tYzUDrdTa"},"outputs":[],"source":["import numpy as np\n","\n","# vocabulary size (add 1 because index 0 is reserved for padding)\n","vocabulary_size = len(tokenizer_inputs.word_index) + 1\n","\n","# dimensionality of GloVe word vectors\n","embedding_dim = len(next(iter(word2vec.values())))\n","\n","# prepare embedding matrix\n","embedding_matrix = np.zeros((vocabulary_size, embedding_dim))\n","for word, i in tokenizer_inputs.word_index.items():\n","    embedding_vector = word2vec.get(word)\n","    if embedding_vector is not None:\n","        # words not found in embedding index will be all-zeros.\n","        embedding_matrix[i] = embedding_vector"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"13hS2nolrh6c"},"outputs":[],"source":["from keras.layers import Embedding\n","\n","embedding_layer = Embedding(input_dim=vocabulary_size,\n","                            output_dim=embedding_dim,\n","                            weights=[embedding_matrix],\n","                            trainable=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1_gjv_1Z9OhI"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MEC7xuMo9OhI"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"3cYWOPxt9OhI"},"source":["### Creating the Model\n","The idea is to have two recurrent neural networks (RNNs) with an encoder-decoder architecture: read the input words one by one to obtain a vector representation of a fixed dimensionality (encoder), and, conditioned on these inputs, extract the output words one by one using another RNN (decoder).\n"]},{"cell_type":"markdown","metadata":{"id":"msNxGTHN9OhI"},"source":["### Define Softmax Activation Function"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LaJI9Cd89OhI"},"outputs":[],"source":["def softmax_over_time(x):\n","  assert(K.ndim(x) \u003e 2)\n","  e = K.exp(x - K.max(x, axis=1, keepdims=True))\n","  s = K.sum(e, axis=1, keepdims=True)\n","  return e / s"]},{"cell_type":"markdown","metadata":{"id":"BEeS4giY9OhJ"},"source":["### Define Embedding Layer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bxIoVAog9OhJ"},"outputs":[],"source":["from keras.layers import Embedding\n","\n","embedding_layer = Embedding(input_dim=vocabulary_size,\n","                            output_dim=embedding_dim,\n","                            weights=[embedding_matrix],\n","                            trainable=False)"]},{"cell_type":"markdown","metadata":{"id":"Fs01n64o9OhJ"},"source":["### One-hot Encoding\n","\n","To make predictions, the final layer of the model will be a dense layer, therefore we need the outputs in the form of one-hot encoded vectors, since we will be using softmax activation function at the dense layer. To create such one-hot encoded output, the next step is to assign 1 to the column number that corresponds to the integer representation of the word."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KimqHY-a9OhJ"},"outputs":[],"source":["\n","decoder_targets_one_hot = np.zeros(\n","  (\n","    len(darija_train),  # changed from input_texts to darija_train\n","    max_len_target,\n","    num_words_output\n","  ),\n","  dtype='float32'\n",")\n","\n","# assign the values\n","for i, d in enumerate(decoder_targets):\n","  for t, word in enumerate(d):\n","    decoder_targets_one_hot[i, t, word] = 1"]},{"cell_type":"markdown","metadata":{"id":"qPNpyfqb9OhJ"},"source":["### Define Encoding Layer\n","The input to the encoder will be the sentence in French and the output will be the hidden state and cell state of the LSTM."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qQ552d2O9OhK"},"outputs":[],"source":["#encoder\n","encoder_inputs_placeholder = Input(shape=(max_len_input,))\n","x = embedding_layer(encoder_inputs_placeholder)\n","encoder = Bidirectional(LSTM(\n","  LATENT_DIM,\n","  return_sequences=True, dropout=0.2\n","))\n","encoder_outputs = encoder(x)\n"]},{"cell_type":"markdown","metadata":{"id":"Az0eJjxF9OhK"},"source":["### Define Decoder Layer\n","\n","The next step is to define the decoder. The decoder will have two inputs: the hidden state and cell state from the encoder and the input sentence, which actually will be the output sentence with an \"\u003c/sos/\u003e\" token appended at the beginning."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xDCoh6GR9OhK"},"outputs":[],"source":["#Decoder\n","decoder_inputs_placeholder = Input(shape=(max_len_target,))\n","decoder_embedding = Embedding(num_words_output, EMBEDDING_DIM)\n","decoder_inputs_x = decoder_embedding(decoder_inputs_placeholder)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"_Xlg9SjS9OhK"},"source":["### Define Attention Layers"]},{"cell_type":"markdown","metadata":{"id":"oHCcJFeJ9OhK"},"source":["To improve the performance of the Neural Machine Translator, Attention mechanism (Dot attention) is implemented. The attention model develops a context vector that is filtered specifically for each output time step.\n","\n","Attention places different focus on different words by assigning each word with a score. Then, using the softmaxed scores, we aggregate the encoder hidden states using a weighted sum of the encoder hidden states, to get the context vector.\n","\n","Rather than re-iterating through the equations for calculating attention, a function `one_step_attention` is defined."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iBw8wtRa9OhK"},"outputs":[],"source":["#Attention\n","attn_repeat_layer = RepeatVector(max_len_input)\n","attn_concat_layer = Concatenate(axis=-1)\n","attn_dense1 = Dense(10, activation='tanh')\n","attn_dense2 = Dense(1, activation=softmax_over_time)\n","\n","attn_dot = Dot(axes=1) # to perform the weighted sum of alpha[t] * h[t]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dhMGm8XT9OhL"},"outputs":[],"source":["def one_step_attention(h, st_1):\n","  # h = h(1), ..., h(Tx), shape = (Tx, LATENT_DIM * 2)\n","  # st_1 = s(t-1), shape = (LATENT_DIM_DECODER,)\n","\n","  # copy s(t-1) Tx times\n","  # now shape = (Tx, LATENT_DIM_DECODER)\n","  st_1 = attn_repeat_layer(st_1)\n","\n","  # Concatenate all h(t)'s with s(t-1)\n","  # Now of shape (Tx, LATENT_DIM_DECODER + LATENT_DIM * 2)\n","  x = attn_concat_layer([h, st_1])\n","\n","  # Neural net first layer\n","  x = attn_dense1(x)\n","\n","  # Neural net second layer with special softmax over time\n","  alphas = attn_dense2(x)\n","\n","  # \"Dot\" the alphas and the h's\n","  # Remember a.dot(b) = sum over a[t] * b[t]\n","  context = attn_dot([alphas, h])\n","\n","  return context\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UAZG501L9OhM"},"outputs":[],"source":["# define the rest of the decoder (after attention)\n","decoder_lstm = LSTM(LATENT_DIM_DECODER, return_state=True)\n","decoder_dense = Dense(num_words_output, activation='softmax')\n","\n","initial_s = Input(shape=(LATENT_DIM_DECODER,), name='s0')\n","initial_c = Input(shape=(LATENT_DIM_DECODER,), name='c0')\n","context_last_word_concat_layer = Concatenate(axis=2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2uAo2VC_9OhM"},"outputs":[],"source":["# s, c will be re-assigned in each iteration of the loop\n","s = initial_s\n","c = initial_c\n","\n","# collect outputs in a list at first\n","outputs = []\n","for t in range(max_len_target): # Ty times\n","  # get the context using attention\n","  context = one_step_attention(encoder_outputs, s)\n","\n","  # we need a different layer for each time step\n","  selector = Lambda(lambda x: x[:, t:t+1])\n","  xt = selector(decoder_inputs_x)\n","\n","  # combine\n","  decoder_lstm_input = context_last_word_concat_layer([context, xt])\n","\n","  # pass the combined [context, last word] into the LSTM\n","  # along with [s, c]\n","  # get the new [s, c] and output\n","  o, s, c = decoder_lstm(decoder_lstm_input, initial_state=[s, c])\n","\n","  # final dense layer to get next word prediction\n","  decoder_outputs = decoder_dense(o)\n","  outputs.append(decoder_outputs)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x-Pk0iHU9OhM"},"outputs":[],"source":["def stack_and_transpose(x):\n","  # x is a list of length T, each element is a batch_size x output_vocab_size tensor\n","  x = K.stack(x) # is now T x batch_size x output_vocab_size tensor\n","  x = K.permute_dimensions(x, pattern=(1, 0, 2)) # is now batch_size x T x output_vocab_size\n","  return x\n","\n","# make it a layerx``\n","stacker = Lambda(stack_and_transpose)\n","outputs = stacker(outputs)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6PGoZCOy9OhM"},"outputs":[],"source":["# create the model\n","model = Model(\n","  inputs=[\n","    encoder_inputs_placeholder,\n","    decoder_inputs_placeholder,\n","    initial_s,\n","    initial_c,\n","  ],\n","  outputs=outputs\n",")"]},{"cell_type":"markdown","metadata":{"id":"r5yiDNjv9OhN"},"source":["### Model Training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e9KVEfUM9OhN"},"outputs":[],"source":["learning_rate=0.001\n","# checkpoint\n","filepath=\"darija_eng_finalmodel\"\n","checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n","callbacks_list = [checkpoint]\n","\n","# compile the model\n","\n","model.compile(optimizer=optimizers.Adam(learning_rate) ,loss='categorical_crossentropy', metrics=['accuracy'])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"80nMUfqC9OhN"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/100\n","7/7 [==============================] - ETA: 0s - loss: 8.0237 - accuracy: 0.6329\n","Epoch 1: val_accuracy improved from -inf to 0.75116, saving model to darija_eng_finalmodel\n","7/7 [==============================] - 128s 12s/step - loss: 8.0237 - accuracy: 0.6329 - val_loss: 6.1635 - val_accuracy: 0.7512\n","Epoch 2/100\n","7/7 [==============================] - ETA: 0s - loss: 4.3509 - accuracy: 0.7522\n","Epoch 2: val_accuracy did not improve from 0.75116\n","7/7 [==============================] - 45s 6s/step - loss: 4.3509 - accuracy: 0.7522 - val_loss: 2.2287 - val_accuracy: 0.7512\n","Epoch 3/100\n","7/7 [==============================] - ETA: 0s - loss: 2.0737 - accuracy: 0.7522\n","Epoch 3: val_accuracy did not improve from 0.75116\n","7/7 [==============================] - 43s 6s/step - loss: 2.0737 - accuracy: 0.7522 - val_loss: 2.0520 - val_accuracy: 0.7512\n","Epoch 4/100\n","7/7 [==============================] - ETA: 0s - loss: 2.0331 - accuracy: 0.7522\n","Epoch 4: val_accuracy did not improve from 0.75116\n","7/7 [==============================] - 42s 6s/step - loss: 2.0331 - accuracy: 0.7522 - val_loss: 2.0328 - val_accuracy: 0.7512\n","Epoch 5/100\n","7/7 [==============================] - ETA: 0s - loss: 1.9904 - accuracy: 0.7522\n","Epoch 5: val_accuracy did not improve from 0.75116\n","7/7 [==============================] - 43s 6s/step - loss: 1.9904 - accuracy: 0.7522 - val_loss: 1.9786 - val_accuracy: 0.7512\n","Epoch 6/100\n","7/7 [==============================] - ETA: 0s - loss: 1.9077 - accuracy: 0.7522\n","Epoch 6: val_accuracy did not improve from 0.75116\n","7/7 [==============================] - 46s 7s/step - loss: 1.9077 - accuracy: 0.7522 - val_loss: 1.8888 - val_accuracy: 0.7512\n","Epoch 7/100\n","7/7 [==============================] - ETA: 0s - loss: 1.8183 - accuracy: 0.7522\n","Epoch 7: val_accuracy did not improve from 0.75116\n","7/7 [==============================] - 45s 6s/step - loss: 1.8183 - accuracy: 0.7522 - val_loss: 1.8166 - val_accuracy: 0.7512\n","Epoch 8/100\n","7/7 [==============================] - ETA: 0s - loss: 1.7617 - accuracy: 0.7522\n","Epoch 8: val_accuracy did not improve from 0.75116\n","7/7 [==============================] - 43s 6s/step - loss: 1.7617 - accuracy: 0.7522 - val_loss: 1.7861 - val_accuracy: 0.7512\n","Epoch 9/100\n","7/7 [==============================] - ETA: 0s - loss: 1.7277 - accuracy: 0.7522\n","Epoch 9: val_accuracy did not improve from 0.75116\n","7/7 [==============================] - 43s 6s/step - loss: 1.7277 - accuracy: 0.7522 - val_loss: 1.7562 - val_accuracy: 0.7512\n","Epoch 10/100\n","7/7 [==============================] - ETA: 0s - loss: 1.7023 - accuracy: 0.7522\n","Epoch 10: val_accuracy did not improve from 0.75116\n","7/7 [==============================] - 43s 6s/step - loss: 1.7023 - accuracy: 0.7522 - val_loss: 1.7413 - val_accuracy: 0.7512\n","Epoch 11/100\n","7/7 [==============================] - ETA: 0s - loss: 1.6887 - accuracy: 0.7522\n","Epoch 11: val_accuracy did not improve from 0.75116\n","7/7 [==============================] - 45s 6s/step - loss: 1.6887 - accuracy: 0.7522 - val_loss: 1.7296 - val_accuracy: 0.7512\n","Epoch 12/100\n","7/7 [==============================] - ETA: 0s - loss: 1.6758 - accuracy: 0.7522\n","Epoch 12: val_accuracy did not improve from 0.75116\n","7/7 [==============================] - 45s 6s/step - loss: 1.6758 - accuracy: 0.7522 - val_loss: 1.7239 - val_accuracy: 0.7512\n","Epoch 13/100\n","7/7 [==============================] - ETA: 0s - loss: 1.6674 - accuracy: 0.7522\n","Epoch 13: val_accuracy did not improve from 0.75116\n","7/7 [==============================] - 51s 7s/step - loss: 1.6674 - accuracy: 0.7522 - val_loss: 1.7199 - val_accuracy: 0.7512\n","Epoch 14/100\n","7/7 [==============================] - ETA: 0s - loss: 1.6608 - accuracy: 0.7522\n","Epoch 14: val_accuracy did not improve from 0.75116\n","7/7 [==============================] - 45s 6s/step - loss: 1.6608 - accuracy: 0.7522 - val_loss: 1.7184 - val_accuracy: 0.7512\n","Epoch 15/100\n","7/7 [==============================] - ETA: 0s - loss: 1.6544 - accuracy: 0.7543\n","Epoch 15: val_accuracy improved from 0.75116 to 0.75737, saving model to darija_eng_finalmodel\n","7/7 [==============================] - 66s 10s/step - loss: 1.6544 - accuracy: 0.7543 - val_loss: 1.7132 - val_accuracy: 0.7574\n","Epoch 16/100\n","7/7 [==============================] - ETA: 0s - loss: 1.6486 - accuracy: 0.7580\n","Epoch 16: val_accuracy did not improve from 0.75737\n","7/7 [==============================] - 45s 7s/step - loss: 1.6486 - accuracy: 0.7580 - val_loss: 1.7146 - val_accuracy: 0.7574\n","Epoch 17/100\n","7/7 [==============================] - ETA: 0s - loss: 1.6456 - accuracy: 0.7580\n","Epoch 17: val_accuracy did not improve from 0.75737\n","7/7 [==============================] - 46s 7s/step - loss: 1.6456 - accuracy: 0.7580 - val_loss: 1.7105 - val_accuracy: 0.7574\n","Epoch 18/100\n","7/7 [==============================] - ETA: 0s - loss: 1.6400 - accuracy: 0.7580\n","Epoch 18: val_accuracy did not improve from 0.75737\n","7/7 [==============================] - 44s 6s/step - loss: 1.6400 - accuracy: 0.7580 - val_loss: 1.7086 - val_accuracy: 0.7574\n","Epoch 19/100\n","7/7 [==============================] - ETA: 0s - loss: 1.6354 - accuracy: 0.7580\n","Epoch 19: val_accuracy improved from 0.75737 to 0.75853, saving model to darija_eng_finalmodel\n","7/7 [==============================] - 69s 10s/step - loss: 1.6354 - accuracy: 0.7580 - val_loss: 1.7072 - val_accuracy: 0.7585\n","Epoch 20/100\n","7/7 [==============================] - ETA: 0s - loss: 1.6317 - accuracy: 0.7591\n","Epoch 20: val_accuracy did not improve from 0.75853\n","7/7 [==============================] - 46s 7s/step - loss: 1.6317 - accuracy: 0.7591 - val_loss: 1.7148 - val_accuracy: 0.7574\n","Epoch 21/100\n","7/7 [==============================] - ETA: 0s - loss: 1.6310 - accuracy: 0.7585\n","Epoch 21: val_accuracy did not improve from 0.75853\n","7/7 [==============================] - 44s 6s/step - loss: 1.6310 - accuracy: 0.7585 - val_loss: 1.7129 - val_accuracy: 0.7574\n","Epoch 22/100\n","7/7 [==============================] - ETA: 0s - loss: 1.6267 - accuracy: 0.7589\n","Epoch 22: val_accuracy did not improve from 0.75853\n","7/7 [==============================] - 47s 7s/step - loss: 1.6267 - accuracy: 0.7589 - val_loss: 1.7037 - val_accuracy: 0.7585\n","Epoch 23/100\n","7/7 [==============================] - ETA: 0s - loss: 1.6216 - accuracy: 0.7591\n","Epoch 23: val_accuracy improved from 0.75853 to 0.75862, saving model to darija_eng_finalmodel\n","7/7 [==============================] - 68s 10s/step - loss: 1.6216 - accuracy: 0.7591 - val_loss: 1.7029 - val_accuracy: 0.7586\n","Epoch 24/100\n","7/7 [==============================] - ETA: 0s - loss: 1.6174 - accuracy: 0.7591\n","Epoch 24: val_accuracy improved from 0.75862 to 0.76031, saving model to darija_eng_finalmodel\n","7/7 [==============================] - 71s 11s/step - loss: 1.6174 - accuracy: 0.7591 - val_loss: 1.7047 - val_accuracy: 0.7603\n","Epoch 25/100\n","7/7 [==============================] - ETA: 0s - loss: 1.6155 - accuracy: 0.7593\n","Epoch 25: val_accuracy did not improve from 0.76031\n","7/7 [==============================] - 47s 7s/step - loss: 1.6155 - accuracy: 0.7593 - val_loss: 1.7085 - val_accuracy: 0.7603\n","Epoch 26/100\n","7/7 [==============================] - ETA: 0s - loss: 1.6170 - accuracy: 0.7597\n","Epoch 26: val_accuracy did not improve from 0.76031\n","7/7 [==============================] - 46s 7s/step - loss: 1.6170 - accuracy: 0.7597 - val_loss: 1.7041 - val_accuracy: 0.7603\n","Epoch 27/100\n","7/7 [==============================] - ETA: 0s - loss: 1.6123 - accuracy: 0.7604\n","Epoch 27: val_accuracy did not improve from 0.76031\n","7/7 [==============================] - 46s 7s/step - loss: 1.6123 - accuracy: 0.7604 - val_loss: 1.7034 - val_accuracy: 0.7602\n","Epoch 28/100\n","7/7 [==============================] - ETA: 0s - loss: 1.6086 - accuracy: 0.7598\n","Epoch 28: val_accuracy did not improve from 0.76031\n","7/7 [==============================] - 45s 7s/step - loss: 1.6086 - accuracy: 0.7598 - val_loss: 1.7119 - val_accuracy: 0.7602\n","Epoch 29/100\n","7/7 [==============================] - ETA: 0s - loss: 1.6105 - accuracy: 0.7609\n","Epoch 29: val_accuracy did not improve from 0.76031\n","7/7 [==============================] - 46s 7s/step - loss: 1.6105 - accuracy: 0.7609 - val_loss: 1.7079 - val_accuracy: 0.7602\n","Epoch 30/100\n","7/7 [==============================] - ETA: 0s - loss: 1.6061 - accuracy: 0.7606\n","Epoch 30: val_accuracy improved from 0.76031 to 0.76286, saving model to darija_eng_finalmodel\n","7/7 [==============================] - 69s 10s/step - loss: 1.6061 - accuracy: 0.7606 - val_loss: 1.7068 - val_accuracy: 0.7629\n","Epoch 31/100\n","7/7 [==============================] - ETA: 0s - loss: 1.6040 - accuracy: 0.7611\n","Epoch 31: val_accuracy did not improve from 0.76286\n","7/7 [==============================] - 46s 7s/step - loss: 1.6040 - accuracy: 0.7611 - val_loss: 1.7011 - val_accuracy: 0.7603\n","Epoch 32/100\n","7/7 [==============================] - ETA: 0s - loss: 1.5988 - accuracy: 0.7616\n","Epoch 32: val_accuracy did not improve from 0.76286\n","7/7 [==============================] - 44s 6s/step - loss: 1.5988 - accuracy: 0.7616 - val_loss: 1.7008 - val_accuracy: 0.7621\n","Epoch 33/100\n","7/7 [==============================] - ETA: 0s - loss: 1.5973 - accuracy: 0.7622\n","Epoch 33: val_accuracy did not improve from 0.76286\n","7/7 [==============================] - 46s 7s/step - loss: 1.5973 - accuracy: 0.7622 - val_loss: 1.7009 - val_accuracy: 0.7620\n","Epoch 34/100\n","7/7 [==============================] - ETA: 0s - loss: 1.5956 - accuracy: 0.7608\n","Epoch 34: val_accuracy did not improve from 0.76286\n","7/7 [==============================] - 46s 7s/step - loss: 1.5956 - accuracy: 0.7608 - val_loss: 1.7011 - val_accuracy: 0.7603\n","Epoch 35/100\n","7/7 [==============================] - ETA: 0s - loss: 1.5936 - accuracy: 0.7619\n","Epoch 35: val_accuracy did not improve from 0.76286\n","7/7 [==============================] - 46s 7s/step - loss: 1.5936 - accuracy: 0.7619 - val_loss: 1.7017 - val_accuracy: 0.7620\n","Epoch 36/100\n","7/7 [==============================] - ETA: 0s - loss: 1.5927 - accuracy: 0.7621\n","Epoch 36: val_accuracy did not improve from 0.76286\n","7/7 [==============================] - 45s 7s/step - loss: 1.5927 - accuracy: 0.7621 - val_loss: 1.7014 - val_accuracy: 0.7602\n","Epoch 37/100\n","7/7 [==============================] - ETA: 0s - loss: 1.5915 - accuracy: 0.7627\n","Epoch 37: val_accuracy improved from 0.76286 to 0.76464, saving model to darija_eng_finalmodel\n","7/7 [==============================] - 69s 10s/step - loss: 1.5915 - accuracy: 0.7627 - val_loss: 1.7019 - val_accuracy: 0.7646\n","Epoch 38/100\n","7/7 [==============================] - ETA: 0s - loss: 1.5913 - accuracy: 0.7634\n","Epoch 38: val_accuracy did not improve from 0.76464\n","7/7 [==============================] - 46s 7s/step - loss: 1.5913 - accuracy: 0.7634 - val_loss: 1.7018 - val_accuracy: 0.7646\n","Epoch 39/100\n","7/7 [==============================] - ETA: 0s - loss: 1.5887 - accuracy: 0.7634\n","Epoch 39: val_accuracy did not improve from 0.76464\n","7/7 [==============================] - 46s 7s/step - loss: 1.5887 - accuracy: 0.7634 - val_loss: 1.7034 - val_accuracy: 0.7620\n","Epoch 40/100\n","7/7 [==============================] - ETA: 0s - loss: 1.5874 - accuracy: 0.7636\n","Epoch 40: val_accuracy did not improve from 0.76464\n","7/7 [==============================] - 46s 7s/step - loss: 1.5874 - accuracy: 0.7636 - val_loss: 1.7015 - val_accuracy: 0.7646\n","Epoch 41/100\n","7/7 [==============================] - ETA: 0s - loss: 1.5868 - accuracy: 0.7630\n","Epoch 41: val_accuracy did not improve from 0.76464\n","7/7 [==============================] - 46s 7s/step - loss: 1.5868 - accuracy: 0.7630 - val_loss: 1.7010 - val_accuracy: 0.7646\n","Epoch 42/100\n","7/7 [==============================] - ETA: 0s - loss: 1.5858 - accuracy: 0.7633\n","Epoch 42: val_accuracy did not improve from 0.76464\n","7/7 [==============================] - 46s 7s/step - loss: 1.5858 - accuracy: 0.7633 - val_loss: 1.7018 - val_accuracy: 0.7646\n","Epoch 43/100\n","7/7 [==============================] - ETA: 0s - loss: 1.5871 - accuracy: 0.7634\n","Epoch 43: val_accuracy did not improve from 0.76464\n","7/7 [==============================] - 46s 7s/step - loss: 1.5871 - accuracy: 0.7634 - val_loss: 1.7041 - val_accuracy: 0.7646\n","Epoch 44/100\n","7/7 [==============================] - ETA: 0s - loss: 1.5852 - accuracy: 0.7632\n","Epoch 44: val_accuracy did not improve from 0.76464\n","7/7 [==============================] - 46s 7s/step - loss: 1.5852 - accuracy: 0.7632 - val_loss: 1.7028 - val_accuracy: 0.7646\n","Epoch 45/100\n","7/7 [==============================] - ETA: 0s - loss: 1.5814 - accuracy: 0.7639\n","Epoch 45: val_accuracy did not improve from 0.76464\n","7/7 [==============================] - 46s 7s/step - loss: 1.5814 - accuracy: 0.7639 - val_loss: 1.7023 - val_accuracy: 0.7646\n","Epoch 46/100\n","7/7 [==============================] - ETA: 0s - loss: 1.5806 - accuracy: 0.7639\n","Epoch 46: val_accuracy did not improve from 0.76464\n","7/7 [==============================] - 46s 7s/step - loss: 1.5806 - accuracy: 0.7639 - val_loss: 1.7035 - val_accuracy: 0.7646\n","Epoch 47/100\n","7/7 [==============================] - ETA: 0s - loss: 1.5788 - accuracy: 0.7639\n","Epoch 47: val_accuracy did not improve from 0.76464\n","7/7 [==============================] - 46s 7s/step - loss: 1.5788 - accuracy: 0.7639 - val_loss: 1.7040 - val_accuracy: 0.7646\n","Epoch 48/100\n","7/7 [==============================] - ETA: 0s - loss: 1.5782 - accuracy: 0.7639\n","Epoch 48: val_accuracy improved from 0.76464 to 0.76469, saving model to darija_eng_finalmodel\n","7/7 [==============================] - 69s 11s/step - loss: 1.5782 - accuracy: 0.7639 - val_loss: 1.7067 - val_accuracy: 0.7647\n","Epoch 49/100\n","7/7 [==============================] - ETA: 0s - loss: 1.5788 - accuracy: 0.7631\n","Epoch 49: val_accuracy did not improve from 0.76469\n","7/7 [==============================] - 44s 6s/step - loss: 1.5788 - accuracy: 0.7631 - val_loss: 1.7051 - val_accuracy: 0.7646\n","Epoch 50/100\n","7/7 [==============================] - ETA: 0s - loss: 1.5769 - accuracy: 0.7639\n","Epoch 50: val_accuracy did not improve from 0.76469\n","7/7 [==============================] - 46s 7s/step - loss: 1.5769 - accuracy: 0.7639 - val_loss: 1.7057 - val_accuracy: 0.7646\n","Epoch 51/100\n","7/7 [==============================] - ETA: 0s - loss: 1.5767 - accuracy: 0.7639\n","Epoch 51: val_accuracy did not improve from 0.76469\n","7/7 [==============================] - 46s 7s/step - loss: 1.5767 - accuracy: 0.7639 - val_loss: 1.7056 - val_accuracy: 0.7646\n","Epoch 52/100\n","7/7 [==============================] - ETA: 0s - loss: 1.5775 - accuracy: 0.7636\n","Epoch 52: val_accuracy did not improve from 0.76469\n","7/7 [==============================] - 46s 7s/step - loss: 1.5775 - accuracy: 0.7636 - val_loss: 1.7057 - val_accuracy: 0.7646\n","Epoch 53/100\n","7/7 [==============================] - ETA: 0s - loss: 1.5785 - accuracy: 0.7637\n","Epoch 53: val_accuracy did not improve from 0.76469\n","7/7 [==============================] - 46s 7s/step - loss: 1.5785 - accuracy: 0.7637 - val_loss: 1.7086 - val_accuracy: 0.7646\n","Epoch 54/100\n","7/7 [==============================] - ETA: 0s - loss: 1.5764 - accuracy: 0.7640\n","Epoch 54: val_accuracy did not improve from 0.76469\n","7/7 [==============================] - 46s 7s/step - loss: 1.5764 - accuracy: 0.7640 - val_loss: 1.7108 - val_accuracy: 0.7646\n","Epoch 55/100\n","7/7 [==============================] - ETA: 0s - loss: 1.5750 - accuracy: 0.7638\n","Epoch 55: val_accuracy did not improve from 0.76469\n","7/7 [==============================] - 46s 7s/step - loss: 1.5750 - accuracy: 0.7638 - val_loss: 1.7155 - val_accuracy: 0.7620\n","Epoch 56/100\n","7/7 [==============================] - ETA: 0s - loss: 1.5740 - accuracy: 0.7637\n","Epoch 56: val_accuracy did not improve from 0.76469\n","7/7 [==============================] - 46s 7s/step - loss: 1.5740 - accuracy: 0.7637 - val_loss: 1.7108 - val_accuracy: 0.7647\n","Epoch 57/100\n","7/7 [==============================] - ETA: 0s - loss: 1.5735 - accuracy: 0.7640\n","Epoch 57: val_accuracy did not improve from 0.76469\n","7/7 [==============================] - 46s 7s/step - loss: 1.5735 - accuracy: 0.7640 - val_loss: 1.7081 - val_accuracy: 0.7646\n","Epoch 58/100\n","7/7 [==============================] - ETA: 0s - loss: 1.5712 - accuracy: 0.7640\n","Epoch 58: val_accuracy did not improve from 0.76469\n","7/7 [==============================] - 46s 7s/step - loss: 1.5712 - accuracy: 0.7640 - val_loss: 1.7087 - val_accuracy: 0.7646\n","Epoch 59/100\n","7/7 [==============================] - ETA: 0s - loss: 1.5722 - accuracy: 0.7640\n","Epoch 59: val_accuracy did not improve from 0.76469\n","7/7 [==============================] - 46s 7s/step - loss: 1.5722 - accuracy: 0.7640 - val_loss: 1.7080 - val_accuracy: 0.7646\n","Epoch 60/100\n","7/7 [==============================] - ETA: 0s - loss: 1.5696 - accuracy: 0.7640\n","Epoch 60: val_accuracy did not improve from 0.76469\n","7/7 [==============================] - 46s 7s/step - loss: 1.5696 - accuracy: 0.7640 - val_loss: 1.7100 - val_accuracy: 0.7647\n","Epoch 61/100\n","7/7 [==============================] - ETA: 0s - loss: 1.5683 - accuracy: 0.7640\n","Epoch 61: val_accuracy did not improve from 0.76469\n","7/7 [==============================] - 46s 7s/step - loss: 1.5683 - accuracy: 0.7640 - val_loss: 1.7088 - val_accuracy: 0.7647\n","Epoch 62/100\n","7/7 [==============================] - ETA: 0s - loss: 1.5673 - accuracy: 0.7639\n","Epoch 62: val_accuracy improved from 0.76469 to 0.76478, saving model to darija_eng_finalmodel\n","7/7 [==============================] - 69s 10s/step - loss: 1.5673 - accuracy: 0.7639 - val_loss: 1.7107 - val_accuracy: 0.7648\n","Epoch 63/100\n","7/7 [==============================] - ETA: 0s - loss: 1.5664 - accuracy: 0.7640\n","Epoch 63: val_accuracy did not improve from 0.76478\n","7/7 [==============================] - 46s 7s/step - loss: 1.5664 - accuracy: 0.7640 - val_loss: 1.7104 - val_accuracy: 0.7646\n","Epoch 64/100\n","7/7 [==============================] - ETA: 0s - loss: 1.5663 - accuracy: 0.7640\n","Epoch 64: val_accuracy did not improve from 0.76478\n","7/7 [==============================] - 46s 7s/step - loss: 1.5663 - accuracy: 0.7640 - val_loss: 1.7100 - val_accuracy: 0.7647\n","Epoch 65/100\n","7/7 [==============================] - ETA: 0s - loss: 1.5654 - accuracy: 0.7640\n","Epoch 65: val_accuracy did not improve from 0.76478\n","7/7 [==============================] - 46s 7s/step - loss: 1.5654 - accuracy: 0.7640 - val_loss: 1.7103 - val_accuracy: 0.7646\n","Epoch 66/100\n","7/7 [==============================] - ETA: 0s - loss: 1.5649 - accuracy: 0.7639\n","Epoch 66: val_accuracy did not improve from 0.76478\n","7/7 [==============================] - 44s 6s/step - loss: 1.5649 - accuracy: 0.7639 - val_loss: 1.7129 - val_accuracy: 0.7646\n","Epoch 67/100\n","7/7 [==============================] - ETA: 0s - loss: 1.5650 - accuracy: 0.7633\n","Epoch 67: val_accuracy did not improve from 0.76478\n","7/7 [==============================] - 46s 7s/step - loss: 1.5650 - accuracy: 0.7633 - val_loss: 1.7133 - val_accuracy: 0.7646\n","Epoch 68/100\n","7/7 [==============================] - ETA: 0s - loss: 1.5666 - accuracy: 0.7639\n","Epoch 68: val_accuracy did not improve from 0.76478\n","7/7 [==============================] - 48s 7s/step - loss: 1.5666 - accuracy: 0.7639 - val_loss: 1.7120 - val_accuracy: 0.7647\n","Epoch 69/100\n","7/7 [==============================] - ETA: 0s - loss: 1.5650 - accuracy: 0.7638\n","Epoch 69: val_accuracy did not improve from 0.76478\n","7/7 [==============================] - 46s 7s/step - loss: 1.5650 - accuracy: 0.7638 - val_loss: 1.7159 - val_accuracy: 0.7647\n","Epoch 70/100\n","7/7 [==============================] - ETA: 0s - loss: 1.5636 - accuracy: 0.7639\n","Epoch 70: val_accuracy improved from 0.76478 to 0.76482, saving model to darija_eng_finalmodel\n","7/7 [==============================] - 68s 10s/step - loss: 1.5636 - accuracy: 0.7639 - val_loss: 1.7177 - val_accuracy: 0.7648\n","Epoch 71/100\n","7/7 [==============================] - ETA: 0s - loss: 1.5656 - accuracy: 0.7636\n","Epoch 71: val_accuracy did not improve from 0.76482\n","7/7 [==============================] - 46s 7s/step - loss: 1.5656 - accuracy: 0.7636 - val_loss: 1.7144 - val_accuracy: 0.7647\n","Epoch 72/100\n","7/7 [==============================] - ETA: 0s - loss: 1.5625 - accuracy: 0.7633\n","Epoch 72: val_accuracy did not improve from 0.76482\n","7/7 [==============================] - 46s 7s/step - loss: 1.5625 - accuracy: 0.7633 - val_loss: 1.7132 - val_accuracy: 0.7646\n","Epoch 73/100\n","7/7 [==============================] - ETA: 0s - loss: 1.5601 - accuracy: 0.7640\n","Epoch 73: val_accuracy did not improve from 0.76482\n","7/7 [==============================] - 46s 7s/step - loss: 1.5601 - accuracy: 0.7640 - val_loss: 1.7175 - val_accuracy: 0.7646\n","Epoch 74/100\n","7/7 [==============================] - ETA: 0s - loss: 1.5632 - accuracy: 0.7637\n","Epoch 74: val_accuracy did not improve from 0.76482\n","7/7 [==============================] - 44s 6s/step - loss: 1.5632 - accuracy: 0.7637 - val_loss: 1.7178 - val_accuracy: 0.7646\n","Epoch 75/100\n","7/7 [==============================] - ETA: 0s - loss: 1.5609 - accuracy: 0.7640\n","Epoch 75: val_accuracy did not improve from 0.76482\n","7/7 [==============================] - 46s 7s/step - loss: 1.5609 - accuracy: 0.7640 - val_loss: 1.7146 - val_accuracy: 0.7647\n","Epoch 76/100\n","7/7 [==============================] - ETA: 0s - loss: 1.5595 - accuracy: 0.7639\n","Epoch 76: val_accuracy did not improve from 0.76482\n","7/7 [==============================] - 47s 7s/step - loss: 1.5595 - accuracy: 0.7639 - val_loss: 1.7146 - val_accuracy: 0.7647\n","Epoch 77/100\n","7/7 [==============================] - ETA: 0s - loss: 1.5585 - accuracy: 0.7639\n","Epoch 77: val_accuracy did not improve from 0.76482\n","7/7 [==============================] - 46s 7s/step - loss: 1.5585 - accuracy: 0.7639 - val_loss: 1.7165 - val_accuracy: 0.7648\n","Epoch 78/100\n","7/7 [==============================] - ETA: 0s - loss: 1.5581 - accuracy: 0.7640\n","Epoch 78: val_accuracy did not improve from 0.76482\n","7/7 [==============================] - 47s 7s/step - loss: 1.5581 - accuracy: 0.7640 - val_loss: 1.7163 - val_accuracy: 0.7647\n","Epoch 79/100\n","7/7 [==============================] - ETA: 0s - loss: 1.5578 - accuracy: 0.7640\n","Epoch 79: val_accuracy did not improve from 0.76482\n","7/7 [==============================] - 45s 6s/step - loss: 1.5578 - accuracy: 0.7640 - val_loss: 1.7165 - val_accuracy: 0.7647\n","Epoch 80/100\n","7/7 [==============================] - ETA: 0s - loss: 1.5570 - accuracy: 0.7639\n","Epoch 80: val_accuracy did not improve from 0.76482\n","7/7 [==============================] - 46s 7s/step - loss: 1.5570 - accuracy: 0.7639 - val_loss: 1.7172 - val_accuracy: 0.7647\n","Epoch 81/100\n","7/7 [==============================] - ETA: 0s - loss: 1.5571 - accuracy: 0.7639\n","Epoch 81: val_accuracy did not improve from 0.76482\n","7/7 [==============================] - 44s 6s/step - loss: 1.5571 - accuracy: 0.7639 - val_loss: 1.7169 - val_accuracy: 0.7647\n","Epoch 82/100\n","7/7 [==============================] - ETA: 0s - loss: 1.5565 - accuracy: 0.7640\n","Epoch 82: val_accuracy did not improve from 0.76482\n","7/7 [==============================] - 46s 7s/step - loss: 1.5565 - accuracy: 0.7640 - val_loss: 1.7175 - val_accuracy: 0.7647\n","Epoch 83/100\n","7/7 [==============================] - ETA: 0s - loss: 1.5565 - accuracy: 0.7640\n","Epoch 83: val_accuracy did not improve from 0.76482\n","7/7 [==============================] - 46s 7s/step - loss: 1.5565 - accuracy: 0.7640 - val_loss: 1.7179 - val_accuracy: 0.7647\n","Epoch 84/100\n","7/7 [==============================] - ETA: 0s - loss: 1.5565 - accuracy: 0.7640\n","Epoch 84: val_accuracy did not improve from 0.76482\n","7/7 [==============================] - 46s 7s/step - loss: 1.5565 - accuracy: 0.7640 - val_loss: 1.7184 - val_accuracy: 0.7647\n","Epoch 85/100\n","7/7 [==============================] - ETA: 0s - loss: 1.5553 - accuracy: 0.7639\n","Epoch 85: val_accuracy did not improve from 0.76482\n","7/7 [==============================] - 46s 7s/step - loss: 1.5553 - accuracy: 0.7639 - val_loss: 1.7202 - val_accuracy: 0.7647\n","Epoch 86/100\n","7/7 [==============================] - ETA: 0s - loss: 1.5547 - accuracy: 0.7639\n","Epoch 86: val_accuracy did not improve from 0.76482\n","7/7 [==============================] - 46s 7s/step - loss: 1.5547 - accuracy: 0.7639 - val_loss: 1.7195 - val_accuracy: 0.7647\n","Epoch 87/100\n","7/7 [==============================] - ETA: 0s - loss: 1.5545 - accuracy: 0.7639\n","Epoch 87: val_accuracy did not improve from 0.76482\n","7/7 [==============================] - 46s 7s/step - loss: 1.5545 - accuracy: 0.7639 - val_loss: 1.7215 - val_accuracy: 0.7648\n","Epoch 88/100\n","7/7 [==============================] - ETA: 0s - loss: 1.5548 - accuracy: 0.7640\n","Epoch 88: val_accuracy did not improve from 0.76482\n","7/7 [==============================] - 46s 7s/step - loss: 1.5548 - accuracy: 0.7640 - val_loss: 1.7202 - val_accuracy: 0.7647\n","Epoch 89/100\n","7/7 [==============================] - ETA: 0s - loss: 1.5532 - accuracy: 0.7639\n","Epoch 89: val_accuracy did not improve from 0.76482\n","7/7 [==============================] - 46s 7s/step - loss: 1.5532 - accuracy: 0.7639 - val_loss: 1.7232 - val_accuracy: 0.7646\n","Epoch 90/100\n","7/7 [==============================] - ETA: 0s - loss: 1.5542 - accuracy: 0.7631\n","Epoch 90: val_accuracy did not improve from 0.76482\n","7/7 [==============================] - 46s 7s/step - loss: 1.5542 - accuracy: 0.7631 - val_loss: 1.7213 - val_accuracy: 0.7646\n","Epoch 91/100\n","7/7 [==============================] - ETA: 0s - loss: 1.5531 - accuracy: 0.7640\n","Epoch 91: val_accuracy did not improve from 0.76482\n","7/7 [==============================] - 46s 7s/step - loss: 1.5531 - accuracy: 0.7640 - val_loss: 1.7229 - val_accuracy: 0.7646\n","Epoch 92/100\n","7/7 [==============================] - ETA: 0s - loss: 1.5541 - accuracy: 0.7634\n","Epoch 92: val_accuracy did not improve from 0.76482\n","7/7 [==============================] - 44s 6s/step - loss: 1.5541 - accuracy: 0.7634 - val_loss: 1.7264 - val_accuracy: 0.7646\n","Epoch 93/100\n","7/7 [==============================] - ETA: 0s - loss: 1.5550 - accuracy: 0.7636\n","Epoch 93: val_accuracy did not improve from 0.76482\n","7/7 [==============================] - 46s 7s/step - loss: 1.5550 - accuracy: 0.7636 - val_loss: 1.7235 - val_accuracy: 0.7648\n","Epoch 94/100\n","7/7 [==============================] - ETA: 0s - loss: 1.5541 - accuracy: 0.7640\n","Epoch 94: val_accuracy did not improve from 0.76482\n","7/7 [==============================] - 46s 7s/step - loss: 1.5541 - accuracy: 0.7640 - val_loss: 1.7243 - val_accuracy: 0.7648\n","Epoch 95/100\n","7/7 [==============================] - ETA: 0s - loss: 1.5530 - accuracy: 0.7639\n","Epoch 95: val_accuracy did not improve from 0.76482\n","7/7 [==============================] - 47s 7s/step - loss: 1.5530 - accuracy: 0.7639 - val_loss: 1.7279 - val_accuracy: 0.7628\n","Epoch 96/100\n","7/7 [==============================] - ETA: 0s - loss: 1.5542 - accuracy: 0.7634\n","Epoch 96: val_accuracy improved from 0.76482 to 0.76487, saving model to darija_eng_finalmodel\n","7/7 [==============================] - 69s 10s/step - loss: 1.5542 - accuracy: 0.7634 - val_loss: 1.7263 - val_accuracy: 0.7649\n","Epoch 97/100\n","7/7 [==============================] - ETA: 0s - loss: 1.5528 - accuracy: 0.7639\n","Epoch 97: val_accuracy did not improve from 0.76487\n","7/7 [==============================] - 46s 7s/step - loss: 1.5528 - accuracy: 0.7639 - val_loss: 1.7242 - val_accuracy: 0.7648\n","Epoch 98/100\n","7/7 [==============================] - ETA: 0s - loss: 1.5520 - accuracy: 0.7637\n","Epoch 98: val_accuracy did not improve from 0.76487\n","7/7 [==============================] - 46s 7s/step - loss: 1.5520 - accuracy: 0.7637 - val_loss: 1.7266 - val_accuracy: 0.7646\n","Epoch 99/100\n","7/7 [==============================] - ETA: 0s - loss: 1.5500 - accuracy: 0.7640\n","Epoch 99: val_accuracy did not improve from 0.76487\n","7/7 [==============================] - 47s 7s/step - loss: 1.5500 - accuracy: 0.7640 - val_loss: 1.7245 - val_accuracy: 0.7647\n","Epoch 100/100\n","7/7 [==============================] - ETA: 0s - loss: 1.5489 - accuracy: 0.7639\n","Epoch 100: val_accuracy did not improve from 0.76487\n","7/7 [==============================] - 46s 6s/step - loss: 1.5489 - accuracy: 0.7639 - val_loss: 1.7251 - val_accuracy: 0.7647\n"]}],"source":["# train the model\n","z = np.zeros((encoder_inputs.shape[0], LATENT_DIM_DECODER)) # initial [s, c]\n","r = model.fit(\n","  [encoder_inputs, decoder_inputs, z, z], decoder_targets_one_hot,\n","  batch_size=BATCH_SIZE,\n","  epochs=EPOCHS,\n","  validation_split=0.2,\n","    callbacks=callbacks_list\n",")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"n-7PzfQS9OhN"},"outputs":[{"data":{"image/png":"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\n","text/plain":["\u003cFigure size 640x480 with 1 Axes\u003e"]},"metadata":{},"output_type":"display_data"}],"source":["# plot some data\n","plt.plot(r.history['loss'], label='loss')\n","plt.plot(r.history['val_loss'], label='val_loss')\n","plt.legend()\n","plt.show()\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"EUIVYv2R9OhO"},"outputs":[{"data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAiwAAAGdCAYAAAAxCSikAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABHWklEQVR4nO3deXwU5eEG8Gf2zH2Tk0AiIHKEw4TEiK0osaiIgMoPEAQppZVD0VgFaoF6QLC2lFapKTRY2qIgiIqCCAY8UCAahBqOAHIEgVyE3Mludvb9/bHJhDWB7IRkJ+Dz/XxGyFz7zpvIPHnnfd+RhBACRERERB2YTusCEBEREbWEgYWIiIg6PAYWIiIi6vAYWIiIiKjDY2AhIiKiDo+BhYiIiDo8BhYiIiLq8BhYiIiIqMMzaF2AtmK323Hu3Dn4+vpCkiSti0NEREQuEEKgoqICkZGR0Oku345y3QSWc+fOITo6WutiEBERUSucOXMGnTt3vuz26yaw+Pr6AnBcsJ+fn8alISIiIleUl5cjOjpauY9fznUTWBoeA/n5+TGwEBERXWNa6s7BTrdERETU4TGwEBERUYfHwEJEREQdHgMLERERdXgMLERERNThMbAQERFRh8fAQkRERB0eAwsRERF1eAwsRERE1OExsBAREVGHx8BCREREHR4DCxEREXV4183LD4moAxMCsNsA2QrYLIBcBwg7oDcBemP9nyZAp+HvUDWljvIp5TEDOj3QwgvZYLcD9rrG65Ktjmu7lJABaxVgqQSsFY6/6wyAZxDgFeT40zMAsMuO4xsWIRxlMZga66il8lxa17IVsFmdz9mwrq7KuUw2K+DhB3gFN5bL5AMYzPV1Uv+nq59vs9R/Xp3ja+edgLoawFJfF9ZKwFbbeI2XLpdeu87QeM6Ga4HU+D0zmACdEZBa+jkSzueQrY66/zGdvv66639OJV1jeRvKDjj/HBs8AJM3YPZx1J/Jx1EutewyUFUMVOYDFfWLtdL5s4yeQLehju/b5c6x79/AxZOAybexXGZfwDPQ+Xst6Zx/Pq3Vjnr6sYgBrbueNsDAQnQteXsScOh953WSHvj5M8Ad85o/pvgY8K/7HP/w/VTpTcDwPwM3T2p++5EtwLoJTYMGUUcXGAtM3Qb4hDqvFwLYOhfIWtG2n/f0UcA3rG3P6SIGFqJrxdl9TcMK4Pjt/bOXgdifAzGDnbfJNuDd33S4sCIgAZIOkmjmt9r2IFth3/IMbJ1vgSn0RudtVReAD55os7AiJD2EpP/ROgl2ozdgdPyGK5l8ALsNoroEupoS6Osqmj0PgDapI7vSSmCCZDBB0hsBozfsRi/YDN6wGrxhlwzwEVXQ1V4EqkuAmouO37Sv8vMFJAidAYCjZUaq/48weMBu9IHd6A3Z4A2b3gRha2z1kGQrdPY66EUd9PY6SPY66Ox1EHqTY9HVtzIAkBqOqd8P4sctA+KS/zr+YtcZIXRGCL0ZQm90tKZAUsonCQFJyI5zylZIsgUQAsLkXV9uL8hGH8dHXdJSo5Mt0NuqobdVQS9brqruAMkRRHzDAZ9wR0uKXAfIdbDV1UKc/x+MF0+i+o1ROHP/ehi9/BHgZUKQtwnY8/fGsNIQ1Otb1ERtOaTai0D1Bcf3+ZIWMGHwRJ3BCxbJA5B00EkSJAnKn5JdgjbtKwwsRNeO7Dccf/YZDdzzSuP6T/4A7P8v8N50YPpXgNkHQggUV1pR9+kriDybDYvBBwfv3oABPbtB11KTvgvKaqzYdqgAm787j+9+KHPpmDoYYIUBdTBAhuNmfOeNwZhxe1ckRHkrj0CqrDZsyP4Ba/bm4UJl8//gB3mb0SvCB73C/XBDqA/qbHZUWGyoqLGhvLYOhRW1yCupwQ8Xq1FrlfEX43L8DDn47rWHMd34EkIDvBHsbYbZoMNvil5CfFUR8s2xSA38K47kV6KmthZG2GBEyzdrAUm5LisMEK3oGmiADb6ohgw9LDCiDgbY68+jgx1G2GCCDUbYIDXTTN8nyh/D+oTjF71D0cnHAzbosTuvAu//rwgfHSpCVa1zGPMy6WGzC1htzuuNegm9I/zQPzYAA6IDcFO4HzoHmOBnFMqjmB9KqrHpwFl8cOAcCisavz9mow49w3zRLdQXBZUyDhXW4mSpTbmO60aV67saYIMXauFtEBg1IAqTbu2KCD9PZfvZ0hr866tT+ODAOdTJdnQN9sLtPTvh9h6dENc5EDqvAJTWCpwtrcHZ0hp8X1SJg+fKcfBsGU5dqEaMdB7rTc+j04WDKP7nGEypexZWGPGbkBzMrUyDBEDc9SLqkmbhm9Ml+Cy3CDvPFOJoQSV8zAaE+JgQHGJCZ686lNbY8F1BHUoqrxzcvxK+iGxl1V0tSYgmUfSaVF5eDn9/f5SVlcHP7zLP84iuVbVlwJ97OfodTPkI6HorjhdWYG3WGXz23QmstsxGJIrxNu7CIunXqKmT0U0+ifdNv4dJkvGUdTretf8MUQGeeDC+M8bEd0Z0kBfsdoEfLtbgSH45jhZUoLjSiopaGypq61BpsaG2Toa/pxFB3mYE+5gQ6GXC/34oRebhQljlxn/YTHodDHoJep0Eo97xW5nu0t/KAEiXBCUhBPLLa2Gv/9dnUEwgfvWzG5BztgyrvzqF8lrHb3x+Hgb4ehiV4+xCoOCS41yhk4A4nwr81/okfKUapNWNxz/kEQCAu3VZSDctg03oMNr6Ar4TNwAADDoJ3UN90DnQE3WygMUmo7bODovNDrtdQEDALhzXIQScfwOF1KSbh2wXqLbKqLLaUGWxoU52XICXSQ8/DyP8PA3wNjt+f7TbBWQhINsd55cuqUsA9UFDRp0sUCfbnepDJwHxXQNxsrgaxZeEPX9PI2yyHVXWpgFMkgDf+s9uqPcf8/MwoHOgF0wGHfafKVXWB3oZ0T3UB4fOlTd7bgAI9TUjwt8DlRabY6m1Kft6GHUwG/QwG3QwG3XwMOjhadLDw6CH2egIOpY6Oyw2GRabHVabHXV2O2yyQJ0sYLM7fgb1kgSdToJecvwM6nSAQaeDTgL0OgkGna7xs4w6GPU61Ml21Fhl1NrsqLXKsMp2yHbRuAhxyffCsQjhOF/DeRvObTLoYNRLMBn0MOgk2OwCtvrzVVtl5JfXKj9XowdGYfTNUdi47yze/fYs5Ppvnl4nKX9vqHNb/fGXE+HvgX76U1ha/Ry8UYNtuAX/sAzDGtNieEh1+LftLrzu9RgqLDIqLc1/b39MJwGxId6IDfGBXQhUWmyottpQVX+OzKdvh98l/0+2BVfv3wwsRGoUHwcKDwK97m+582FbyloJbPkt7CE98U7SBqz75gd8c/qisjlZdxBvmRYBACZZ52C3vQ82mX6PXro87DUn45+RL2LvyRKnG1L3UB+cK6254j+IV9I7wg8P3ByF+wdEItTXQ/Xxp4qr8I/Pv8c72Wedwg8A3NDJG4/d3g2jBkTBZHD+Db3GKuPQ+XLknC3Dd2fLcKq4Cl5mA/w8DPDzNMLPw4hOvmbEhnghJthbudGKb/8L6f2ZsOtM2HvXRhTa/TDss5HwsF5Edpdf4suuM9A50BO9IvzQrZNPk89tSxabDJ3kCHdXq6jCgs3/O4dNB85hX16psj7Qy4j7+kVi1MBI3NwlEJIkoU62o6LWhvKaOhj0Evw8jfAxGaDTSRDCEV73nynFgTOl2H+mFCeLq3Chyur0eZIE/KxHJ4xNiEZK71CYDXrY7QInL1Thux/KcKywAqG+HugZ7oueYb4I9G76AMFuF45w587/hzQihMDuExewfOdxfHn8QpPtP+sRgsfv7IFeEb747GgRPjlUgB1HCp3+X+3ka0ZkgCe6BHmhT6Rf/eLvePQDACc+A9Y8BMhWCEkPScg44HULHq54HFV1jjoO8THh5zd2wh09QzEoJgg1dTKKKy24UGlBUaUV3iY9eob7olsnH3gY9U3K2Z4YWIjaWnUJ8PdbgMoC4O4lwC3T3fKxdTYZlleT4VOWi8X2R7HC+gsAjt/I7rwpFP+XEI3YEG+EfDEfAd+tgs07HDXdh8P3QIZjFMCMPYBPKGrrZGw7VID135zBruPFymN+k0GH7p18cFO4LyICPODrYYSP2QBfDwPMBh3KaupwocqKkkorSqqs6ORnxqgBUegV0Tb/n+WX1eKfX5zAu9+eRVSgJ6bf3g2/6BMOva6Nb2ZCAG+NB45+BIT3A4JiHX2CQnsDv/7UMRrmGnempBqfHi1CVIAHftajU5sEomqrDWcv1uCHizW4UGVFcrdgRAV4tnwgNbEv7yL+vvM4dh0vxuBuIZh1Z3cM7BLYZL862Y7c/Ap4mw2I8PdwLUAcfA9Y/ygA4RjJM2ULaiUPfHPqIvw8Degb6Q9dW/8/1UYYWIja2oZfAjnvOP6uNztucmG9m+534Xvg0HuAwRMWkz+yiyTsOG2DFB6Hp+/td9l/fETZDyitkXFWDnA8s75Yg6MFFTif8xlWi9+jRpiQZHkNAUGhGDsoGg/Fd0aY3yUtG9YqIP02oORE47oxq4E+o5p81tnSGhw+V46YEG/EBHvB0AY3tmtCRQHw9yRHR0PAMcJqWiYQOVDbchG1hYPvAce3A3cu0GwkT2swsBC1pZyNwIYpjhtc5ADgbDYQ1heYtsP5N/PzByBW3w+ptrTJKX4QIVgU+BKemzwSnQO9nE+/8230+GwmhBD4Zd0z+MreV9n2Z+PreFD/Bb4NHg4xcjkGRgdcvik9by/wxt2OES99HwIeymiDi7/O5LzjCJ8A8LPfAkPna1seop84BhaitlJZCCxPAmpKgJ8/CyROA/6eDFQXA7c+jouDF2Bf3kX8cGg3HsyZCR9RicP2LjguIhGASoQZqxEtFcNTLscF4YsndM9h+sNjcFuPEOSX1eLDt/6OyedfglFy9CWphQmL/ReiODQZ3X3r8OT+EdDZrcCvMoHOCS2X9+t/Aie/AO77i2NCKGpqZxpQ/gMwfOl18SiI6FrWroFl+fLleOWVV5Cfn4/+/fvj1VdfRWJiYrP7DhkyBJ999lmT9ffeey82b96sfH348GHMmTMHn332GWw2G3r37o133nkHXbp0calMDCzU1grLa7Fmz2mMyn0GsRc+gzWkDwy/2Qm7zohTX61H98xfww4JE62/Q5nwwhrTYgRIVdhn745ZuvkY0r8bHrw5ytHhsfoCrKsfgKnwACqFB35d9zTCB/wCnjlv4QXpH9BLAjmBd6FnkATj99scs2WOXwsUHgY+ngeExwG/+cK9HX2JiNyg3QLLunXrMGnSJKSnpyMpKQnLli3D+vXrkZubi9DQ0Cb7l5SUwGpt7GV+4cIF9O/fH//85z/x6KOPAgC+//57JCYmYurUqRg/fjz8/Pxw8OBB3HLLLc2e82oumAhw9NwvrLAgxMfcpHOnEALv7T+LP2w6hKGWTCw1pcMq9BhhXYTThhgYdTpUWGxYbPgnHjbsQL4IhJdUBz9UoiigP8oeXIsboiKadnCzVEB+62HoT30OizDgHflneNiwEwBw8aZxCPy/vzsmcHp7sqNjqMED8AhwTPp231+AhF+6qXaIiNyn3QJLUlISBg0ahNdeew0AYLfbER0djccffxxz585t8fhly5ZhwYIFOH/+PLy9vQEA48aNg9FoxH/+8x81RXHCwPITdeF7R8/4oiMt76szQjZ6o0KYUWQxosRmgtCbEeznjdBAf/j5eMFiE8jJK8LFiiqYUYcE/XF4oQarPCdjScU9ykRb/p5GDO3mjefPT4dv1WnH+TsnAhPfufx7PQCgrhbinamQjnyorBJJMyDdvbix9cRmaQwtgONdJE8fcbz/g4joOtMugcVqtcLLywsbNmzAqFGjlPWTJ09GaWkp3n+/mWnDfyQuLg7JyclYscIxZbDdboe/vz+effZZ7Nq1C99++y1iY2Mxb948p8/4MYvFAoulcWKk8vJyREdHM7D8lBQcAv4zyjHMuD11HgRM2QobdMgrqUa1VUavCD9Hy8zZbOA/DwAR/YCxa64cVhrINmDLb4Fv/+Po9DlkbtNHPTar471BRz8CEn8N3PtK8+ciIrrGuRpYVE3NX1xcDFmWERbmPFwqLCwMR460/BtuVlYWcnJykJHROHKhsLAQlZWVWLJkCV566SW8/PLL2Lp1Kx544AHs3LkTt99+e7PnSktLw/PPP6+m+HQ9OZsN/PdBx/DUsL7AQ6tQq/PE+dIa/HCxFqcuVOF/PzgmFiutdjySNEo2+KAWgyKNuKubNwaGG3EyvwQHThXh2PkS6OxW6CAQHOCL0QmxiAjyd3TIvGEIoDfAAOCGTj7O5YiKB357VF3HTb0BGLHMMZeL8TITrhlMwNj/Anm7XetoS0R0nXPru4QyMjIQFxfn1EHXXj+18siRI/HUU08BAAYMGICvvvoK6enplw0s8+bNQ2pqqvJ1QwsLXRs+P1qE/+w5jRAfM+68KRSDuwfDy+T841habUVufgXOXKxBflkNzpXV4nxpDcJL92F++R/gJWqQa+iJP1h/j5Mrf1Cmv3bmC5NBh/6d/XFrtxA8cHMUugZ7K1vj6peK2jpsP1QAnSRheL8IdRNutXaUyeXCSgO9AYj9WevOTUR0nVEVWEJCQqDX61FQ4NwEX1BQgPDw8CseW1VVhbVr1+KFF15ock6DwYDevZ0n4OrVqxd27dp12fOZzWaYzRyOeM049y1QkY/vA2/Doi1HsONIobLpraw8mAw63HJDMG4K98Wxggocya9AQVk17tFloYtUCG+pBj1Qi5ulGgzX7YGnZMVuuTd+Vfs0qirtABxhxddsQJdgx5Ts/aP9Ed81CH2j/GA2XHmmSF8PIx64uXN71gAREV0FVYHFZDIhPj4emZmZSv8Su92OzMxMzJo164rHrl+/HhaLBRMnTmxyzkGDBiE3N9dp/dGjR9G1a1c1xaMORAgBq2xHTUUZjJ++AO8DjjcNF9j74Fjdr2DQhWPiLV1hFwI7jhTih4s1+PxoET4/WgQAuFE6g9dNKzBA932z5y+OHALLrX/DqwZHK0WAlwldg7wQ5G36SbyfhIjop0b1I6HU1FRMnjwZCQkJSExMxLJly1BVVYUpU6YAACZNmoSoqCikpaU5HZeRkYFRo0YhODi4yTmfeeYZjB07Fj//+c9xxx13YOvWrfjggw/w6aeftu6qqN28v/8s5m38rslr6X/MLgR+Lu3HImMGoiTHC78swoBbdQfxicc8VN46B8FDhwE6PZ6/X+B4YSV2HClEQUk5RlauRdzJDOjsdYDZD7jpPscIGbOPY8SMfzRCeo/EEEPTl6oREdH1SXVgGTt2LIqKirBgwQLk5+djwIAB2Lp1q9IRNy8vDzqd8/P/3Nxc7Nq1C9u2bWv2nKNHj0Z6ejrS0tLwxBNPoGfPnnjnnXdw2223teKSqE3lfgQUHgLgeK392c9P4FHZBrTQiHGTPg/363cDAM6IULwgPQZdcBe8bFiJgMK9MH/5PHBqM3DTvZAA9ADQQxLAuQ1A0WHHSXreCwz/M+AX2X7XR0RE1wROzU+XV3wMeK31I1QEJNiTpkM/9PeAqb6jq90OfPtvYNt8wFLe/IFeIY5hvH1Gc2ZXIqLrXLsMa6afmKNbHX8GxkLuehu2fHceVRYbbu4SiBvDfK58rMEMqf946H88JFenA+IfBXr8AtjzeuNbcxv4hAHJM/kOHCIicsLAQpd3rP4RXtJvsEE/HHP2fIcQHzN2TbkDMF551E2L/CKBX7x49WUkIqKfBBWTTdBPiqUCOO3og2K7IQXLdzpG6zx2+w3wuNqwQkREpBIDCzXvxGeAvQ4IugHvn/FEXkk1grxNeDjJtbdnExERtSUGFmpe/eMge/e7sHzncQDAtJ/d0GQ2WiIiIndgYKGmhACObQcA7NXfjBPFVQjwMuKRZE7kR0RE2mBgoaYKDgIV5yAMnngpxzFaZ+rgWPiY2bpCRETaYGChpo47WleOeg3EwaI6+HkYMHlwjLZlIiKinzQGFmqq/nHQfy70BAC8MqY//DyMWpaIiIh+4hhYyFlNKUTeHgDAp/b+ePbunhjW58pv4iYiImpvDCzkJH//R5CEjOP2SCQOGIjpt3fTukhERESc6bY1ThZXoajConUx2pxsFyjNXI97ABzyuQVpD8ZB4rt8iIioA2BgUWn/mVKMWv6l1sVoFxLs2GvOBiTg9uEPw2zgjLZERNQxMLCodKq4CgDgYdQh0t9T49K0rR7y9witKYXd6A3/nj/XujhEREQKBpaWvD8TqClVvhxUWot0YxlCfExI6ByoXbnaQ2keUAPout0BGMxal4aIiEjBwNKSY58AlfnKl1EAovQAagAc0apQ7aznvVqXgIiIyAkDS0vueh6oq1a+3HuyBO/vP4feEb6YeMt1OFW9RwDQe6TWpSAiInLCwNKS/uOcvsytO4U3sw/i3sBwTEyI16hQREREPy2ch0UlmywAAHodq46IiMhdeNdVyS4cgcWg4/wkRERE7sLAopLN3tDCwsBCRETkLgwsKsl2trAQERG5GwOLSo19WBhYiIiI3IWBRSXZbgfAwEJERORODCwqsQ8LERGR+zGwqCRzlBAREZHbMbCoJHMeFiIiIrfjXVclG0cJERERuR0Di0oy+7AQERG5HQOLSmxhISIicj8GFpUahjXrGFiIiIjchoFFJdmRV9jCQkRE5EYMLCpx4jgiIiL3Y2BRiX1YiIiI3I+BRSVllJCeVUdEROQuvOuqxBYWIiIi92NgUUlpYZEYWIiIiNyFgUUlThxHRETkfgwsKjUEFoOegYWIiMhdGFhUsnFYMxERkdu1KrAsX74cMTEx8PDwQFJSErKysi6775AhQyBJUpNl+PDhze7/2GOPQZIkLFu2rDVFa3cyO90SERG5nerAsm7dOqSmpmLhwoXYt28f+vfvj2HDhqGwsLDZ/Tdu3Ijz588rS05ODvR6PcaMGdNk33fffRd79uxBZGSk+itxE5vSh4WNU0RERO6i+q67dOlSTJs2DVOmTEHv3r2Rnp4OLy8vrFq1qtn9g4KCEB4erizbt2+Hl5dXk8By9uxZPP7441izZg2MRmPrrsYNGjvdalwQIiKinxBVt12r1Yrs7GykpKQ0nkCnQ0pKCnbv3u3SOTIyMjBu3Dh4e3sr6+x2Ox555BE888wz6NOnj0vnsVgsKC8vd1rcQWYLCxERkdupuusWFxdDlmWEhYU5rQ8LC0N+fn6Lx2dlZSEnJwe/+tWvnNa//PLLMBgMeOKJJ1wuS1paGvz9/ZUlOjra5WOvBvuwEBERuZ9bmwkyMjIQFxeHxMREZV12djb++te/4l//+hckFZOxzZs3D2VlZcpy5syZ9ihyEzbOw0JEROR2qgJLSEgI9Ho9CgoKnNYXFBQgPDz8isdWVVVh7dq1mDp1qtP6L774AoWFhejSpQsMBgMMBgNOnz6Np59+GjExMZc9n9lshp+fn9PiDmxhISIicj9VgcVkMiE+Ph6ZmZnKOrvdjszMTCQnJ1/x2PXr18NisWDixIlO6x955BH873//w/79+5UlMjISzzzzDD7++GM1xXMLzsNCRETkfga1B6SmpmLy5MlISEhAYmIili1bhqqqKkyZMgUAMGnSJERFRSEtLc3puIyMDIwaNQrBwcFO64ODg5usMxqNCA8PR8+ePdUWr93JMh8JERERuZvqwDJ27FgUFRVhwYIFyM/Px4ABA7B161alI25eXh50PxpBk5ubi127dmHbtm1tU2oNsQ8LERGR+0lCCKF1IdpCeXk5/P39UVZW1q79WRJe2o7iSis+fvLn6Bnu226fQ0RE9FPg6v2bk4moxBYWIiIi92NgUamhDwtHCREREbkPA4tKbGEhIiJyPwYWlZR5WPQMLERERO7CwKKSMg+Lill5iYiI6OowsKgghEB9AwsfCREREbkRA4sKDY+DAMDAtzUTERG5De+6KtguCSx69mEhIiJyGwYWFZxbWBhYiIiI3IWBRQWnFhYGFiIiIrdhYFHh0hYWjhIiIiJyHwYWFRoCiyQBOrawEBERuQ0DiwrKpHEMK0RERG7FwKKCMmkcAwsREZFbMbCo0NjCwmojIiJyJ955VeCLD4mIiLTBwKKCzMBCRESkCQYWFRhYiIiItMHAogJHCREREWmDgUUF9mEhIiLSBgOLCnL9sGa2sBAREbkXA4sKNpktLERERFpgYFGBnW6JiIi0wcCiQmMfFlYbERGRO/HOq4IsOEqIiIhICwwsKsjsw0JERKQJBhYVbJyHhYiISBMMLCqw0y0REZE2GFhUsNXPw8LAQkRE5F4MLCqwhYWIiEgbDCwq8F1CRERE2mBgUUHmPCxERESa4J1XBY4SIiIi0gYDiwpKC4uegYWIiMidGFhUYAsLERGRNhhYVJAbhjVLDCxERETuxMCiguzIKxzWTERE5GYMLCo0tLAY2IeFiIjIrRhYVLBx4jgiIiJNtCqwLF++HDExMfDw8EBSUhKysrIuu++QIUMgSVKTZfjw4QCAuro6zJkzB3FxcfD29kZkZCQmTZqEc+fOte6K2lHjxHHMeURERO6k+s67bt06pKamYuHChdi3bx/69++PYcOGobCwsNn9N27ciPPnzytLTk4O9Ho9xowZAwCorq7Gvn37MH/+fOzbtw8bN25Ebm4u7r///qu7snbAFhYiIiJtGNQesHTpUkybNg1TpkwBAKSnp2Pz5s1YtWoV5s6d22T/oKAgp6/Xrl0LLy8vJbD4+/tj+/btTvu89tprSExMRF5eHrp06aK2iO2G7xIiIiLShqoWFqvViuzsbKSkpDSeQKdDSkoKdu/e7dI5MjIyMG7cOHh7e192n7KyMkiShICAADXFa3cMLERERNpQ1cJSXFwMWZYRFhbmtD4sLAxHjhxp8fisrCzk5OQgIyPjsvvU1tZizpw5GD9+PPz8/C67n8VigcViUb4uLy934QquDl9+SEREpA239h7NyMhAXFwcEhMTm91eV1eH//u//4MQAq+//voVz5WWlgZ/f39liY6Obo8iO7E1TBzHwEJERORWqgJLSEgI9Ho9CgoKnNYXFBQgPDz8isdWVVVh7dq1mDp1arPbG8LK6dOnsX379iu2rgDAvHnzUFZWpixnzpxRcymtwhYWIiIibagKLCaTCfHx8cjMzFTW2e12ZGZmIjk5+YrHrl+/HhaLBRMnTmyyrSGsHDt2DJ988gmCg4NbLIvZbIafn5/T0t5sckMfFg5rJiIicifVo4RSU1MxefJkJCQkIDExEcuWLUNVVZUyamjSpEmIiopCWlqa03EZGRkYNWpUkzBSV1eHhx56CPv27cOHH34IWZaRn58PwDHCyGQytfba2lxjp1uNC0JERPQTozqwjB07FkVFRViwYAHy8/MxYMAAbN26VemIm5eXB92PWiByc3Oxa9cubNu2rcn5zp49i02bNgEABgwY4LRt586dGDJkiNoitpvGeViYWIiIiNxJdWABgFmzZmHWrFnNbvv000+brOvZsyeEEM3uHxMTc9ltHY0s2IeFiIhIC2wqUEGWOQ8LERGRFhhYVLBxlBAREZEmGFhUkDkPCxERkSYYWFTgyw+JiIi0wcCiAt8lREREpA0GFhUaZ7pltREREbkT77wqsIWFiIhIGwwsKnCUEBERkTYYWFRQWlj0DCxERETuxMCiAltYiIiItMHAooIyD4vEwEJERORODCwqsNMtERGRNhhYVFCGNbMPCxERkVsxsKjQONMtq42IiMideOdVQWanWyIiIk0wsKjAdwkRERFpg4FFBXa6JSIi0gYDiwoMLERERNpgYFGBfViIiIi0wcCigq1h4jgGFiIiIrdiYFGhsYWF1UZEROROvPOqwFFCRERE2mBgcZHdLiAceYWBhYiIyM0YWFzU0LoCMLAQERG5GwOLi+yiMbBwlBAREZF7MbC4iC0sRERE2mFgcZEss4WFiIhIKwwsLmqYgwVgCwsREZG7MbC4qGEOFp0ESBIDCxERkTsxsLjIxknjiIiINMO7r4v44kMiIiLtMLC4iC8+JCIi0g4Di4uUafn1DCxERETuxsDiIrawEBERaYeBxUUNw5rZh4WIiMj9GFhcpHS65ZBmIiIit2NgcZHMPixERESaYWBxkcx5WIiIiDTDu6+LbJyHhYiISDMMLC7iKCEiIiLttCqwLF++HDExMfDw8EBSUhKysrIuu++QIUMgSVKTZfjw4co+QggsWLAAERER8PT0REpKCo4dO9aaorUbtrAQERFpR3VgWbduHVJTU7Fw4ULs27cP/fv3x7Bhw1BYWNjs/hs3bsT58+eVJScnB3q9HmPGjFH2+eMf/4i//e1vSE9Px969e+Ht7Y1hw4ahtra29VfWxmQOayYiItKM6sCydOlSTJs2DVOmTEHv3r2Rnp4OLy8vrFq1qtn9g4KCEB4erizbt2+Hl5eXEliEEFi2bBl+//vfY+TIkejXrx/+/e9/49y5c3jvvfeu6uLakuzIKwwsREREGlAVWKxWK7Kzs5GSktJ4Ap0OKSkp2L17t0vnyMjIwLhx4+Dt7Q0AOHnyJPLz853O6e/vj6SkJJfP6Q4NLSzsw0JEROR+BjU7FxcXQ5ZlhIWFOa0PCwvDkSNHWjw+KysLOTk5yMjIUNbl5+cr5/jxORu2NcdiscBisShfl5eXu3QNrcU+LERERNpx6yihjIwMxMXFITEx8arPlZaWBn9/f2WJjo5ugxJeHudhISIi0o6qu29ISAj0ej0KCgqc1hcUFCA8PPyKx1ZVVWHt2rWYOnWq0/qG49Sec968eSgrK1OWM2fOqLkU1WwyW1iIiIi0oiqwmEwmxMfHIzMzU1lnt9uRmZmJ5OTkKx67fv16WCwWTJw40Wl9bGwswsPDnc5ZXl6OvXv3XvGcZrMZfn5+Tkt7kvlIiIiISDOq+rAAQGpqKiZPnoyEhAQkJiZi2bJlqKqqwpQpUwAAkyZNQlRUFNLS0pyOy8jIwKhRoxAcHOy0XpIkPPnkk3jppZfQo0cPxMbGYv78+YiMjMSoUaNaf2VtjH1YiIiItKM6sIwdOxZFRUVYsGAB8vPzMWDAAGzdulXpNJuXlwfdj/p55ObmYteuXdi2bVuz53z22WdRVVWFX//61ygtLcVtt92GrVu3wsPDoxWX1D5kwZluiYiItCIJUX8nvsaVl5fD398fZWVl7fJ46F9fnsQfPjiE+/pF4LWHb27z8xMREf0UuXr/5pAXF9n4LiEiIiLNMLC4qLHTLauMiIjI3Xj3dVFjp1uNC0JERPQTxNuvi9jCQkREpB3efV0ksw8LERGRZhhYXMSJ44iIiLTDwOIijhIiIiLSDgOLi2S7HQCg1zOwEBERuRsDi4uUUUISAwsREZG7MbC4iJ1uiYiItMPA4iIOayYiItIO774uUlpY2IeFiIjI7RhYXGTjsGYiIiLNMLC4iH1YiIiItMPA4iK2sBAREWmHgcVFyjwsDCxERERux8DiIk7NT0REpB0GFhexDwsREZF2GFhcZOM8LERERJrh3ddFbGEhIiLSDgOLi2wy+7AQERFphYHFRex0S0REpB0GFhfJgoGFiIhIKwwsLrKxDwsREZFmGFhcxInjiIiItMPA4qKGTrcGDmsmIiJyO959XcROt0RERNphYHERAwsREZF2GFhcxLc1ExERaYeBxUWc6ZaIiEg7DCwu4iMhIiIi7TCwuEiZh0XPwEJERORuDCwuapiHhY+EiIiI3I+BxUUNLSw6iYGFiIjI3RhYXNTY6ZZVRkRE5G68+7pI6XTLPixERERux8DiIg5rJiIi0g4DiwuEEJw4joiISEMMLC6ozyoA2MJCRESkBQYWF9jqhzQDbGEhIiLSQqsCy/LlyxETEwMPDw8kJSUhKyvrivuXlpZi5syZiIiIgNlsxo033ogtW7Yo22VZxvz58xEbGwtPT09069YNL774IoQQVzir+8iXNLEwsBAREbmfQe0B69atQ2pqKtLT05GUlIRly5Zh2LBhyM3NRWhoaJP9rVYr7rrrLoSGhmLDhg2IiorC6dOnERAQoOzz8ssv4/XXX8fq1avRp08ffPPNN5gyZQr8/f3xxBNPXNUFtgUGFiIiIm2pDixLly7FtGnTMGXKFABAeno6Nm/ejFWrVmHu3LlN9l+1ahVKSkrw1VdfwWg0AgBiYmKc9vnqq68wcuRIDB8+XNn+1ltvtdhy4y6XBhbOw0JEROR+qu6+VqsV2dnZSElJaTyBToeUlBTs3r272WM2bdqE5ORkzJw5E2FhYejbty8WL14MWZaVfW699VZkZmbi6NGjAIADBw5g165duOeeey5bFovFgvLycqelvdguCSxsYCEiInI/VS0sxcXFkGUZYWFhTuvDwsJw5MiRZo85ceIEduzYgQkTJmDLli04fvw4ZsyYgbq6OixcuBAAMHfuXJSXl+Omm26CXq+HLMtYtGgRJkyYcNmypKWl4fnnn1dT/Fa7dA4WiVPzExERuV27P9+w2+0IDQ3FihUrEB8fj7Fjx+K5555Denq6ss/bb7+NNWvW4M0338S+ffuwevVq/OlPf8Lq1asve9558+ahrKxMWc6cOdNu18A5WIiIiLSlqoUlJCQEer0eBQUFTusLCgoQHh7e7DEREREwGo3Q6/XKul69eiE/Px9WqxUmkwnPPPMM5s6di3HjxgEA4uLicPr0aaSlpWHy5MnNntdsNsNsNqspfqvJMgMLERGRllS1sJhMJsTHxyMzM1NZZ7fbkZmZieTk5GaPGTx4MI4fPw77JXOZHD16FBERETCZTACA6upq6H7UmVWv1zsdoyVZMLAQERFpSfUjodTUVKxcuRKrV6/G4cOHMX36dFRVVSmjhiZNmoR58+Yp+0+fPh0lJSWYPXs2jh49is2bN2Px4sWYOXOmss+IESOwaNEibN68GadOncK7776LpUuXYvTo0W1wiVdPrg9OnOWWiIhIG6qHNY8dOxZFRUVYsGAB8vPzMWDAAGzdulXpiJuXl+fUWhIdHY2PP/4YTz31FPr164eoqCjMnj0bc+bMUfZ59dVXMX/+fMyYMQOFhYWIjIzEb37zGyxYsKANLvHqNfZh4ZBmIiIiLUiio0wne5XKy8vh7++PsrIy+Pn5tem5c86W4b5XdyHczwN7fje0Tc9NRET0U+bq/ZtNBi6QOUqIiIhIUwwsLuCwZiIiIm0xsLjg0onjiIiIyP0YWFzAR0JERETaYmBxAQMLERGRthhYXGBrmIdFz8BCRESkBQYWF8ich4WIiEhTvAO7QBklxAYWIiIiTTCwuKBxlBCri4iISAu8A7uAnW6JiIi0xcDiAqWFhc+EiIiINMHA4gLOdEtERKQtBhYXyA3DmhlYiIiINMHA4oKGFhadxMBCRESkBQYWF7APCxERkbYYWFzAieOIiIi0xTuwC/i2ZiIiIm0xsLiAo4SIiIi0xcDiArawEBERaYuBxQU2mS0sREREWmJgcUHDPCwMLERERNpgYHGBLNjCQkREpCUGFhfY2IeFiIhIUwwsLpBlzsNCRESkJd6BXcAWFiIiIm0xsLhA5jwsREREmmJgcQEnjiMiItIWA4sLOKyZiIhIWwwsLpAdeYV9WIiIiDTCwOICtrAQERFpi4HFBRwlREREpC0GFhcoo4T0rC4iIiIt8A7sAmWUkMQWFiIiIi0wsLhA5iMhIiIiTTGwuIATxxEREWmLgcUFSguLnoGFiIhICwwsLrBxWDMREZGmGFhcwD4sRERE2mJgcUHDKCEdRwkRERFpgoHFBezDQkREpK1WBZbly5cjJiYGHh4eSEpKQlZW1hX3Ly0txcyZMxEREQGz2Ywbb7wRW7Zscdrn7NmzmDhxIoKDg+Hp6Ym4uDh88803rSlem2scJcR8R0REpAWD2gPWrVuH1NRUpKenIykpCcuWLcOwYcOQm5uL0NDQJvtbrVbcddddCA0NxYYNGxAVFYXTp08jICBA2efixYsYPHgw7rjjDnz00Ufo1KkTjh07hsDAwKu6uLbCPixERETaUh1Yli5dimnTpmHKlCkAgPT0dGzevBmrVq3C3Llzm+y/atUqlJSU4KuvvoLRaAQAxMTEOO3z8ssvIzo6Gm+88YayLjY2Vm3R2o2N87AQERFpStUzDqvViuzsbKSkpDSeQKdDSkoKdu/e3ewxmzZtQnJyMmbOnImwsDD07dsXixcvhizLTvskJCRgzJgxCA0NxcCBA7Fy5corlsVisaC8vNxpaS9sYSEiItKWqsBSXFwMWZYRFhbmtD4sLAz5+fnNHnPixAls2LABsixjy5YtmD9/Pv785z/jpZdectrn9ddfR48ePfDxxx9j+vTpeOKJJ7B69erLliUtLQ3+/v7KEh0dreZSVOE8LERERNpS/UhILbvdjtDQUKxYsQJ6vR7x8fE4e/YsXnnlFSxcuFDZJyEhAYsXLwYADBw4EDk5OUhPT8fkyZObPe+8efOQmpqqfF1eXt5uoUWW+UiIiIhIS6oCS0hICPR6PQoKCpzWFxQUIDw8vNljIiIiYDQaodfrlXW9evVCfn4+rFYrTCYTIiIi0Lt3b6fjevXqhXfeeeeyZTGbzTCbzWqK32qyYGAhIiLSkqpHQiaTCfHx8cjMzFTW2e12ZGZmIjk5udljBg8ejOPHj8Ne/1gFAI4ePYqIiAiYTCZln9zcXKfjjh49iq5du6opXrtp7MPCYc1ERERaUH0HTk1NxcqVK7F69WocPnwY06dPR1VVlTJqaNKkSZg3b56y//Tp01FSUoLZs2fj6NGj2Lx5MxYvXoyZM2cq+zz11FPYs2cPFi9ejOPHj+PNN9/EihUrnPbREkcJERERaUt1H5axY8eiqKgICxYsQH5+PgYMGICtW7cqHXHz8vKgu6QlIjo6Gh9//DGeeuop9OvXD1FRUZg9ezbmzJmj7DNo0CC8++67mDdvHl544QXExsZi2bJlmDBhQhtc4tVr6MPCUUJERETakISo76BxjSsvL4e/vz/Kysrg5+fXpufuNX8raupkfPHsHYgO8mrTcxMREf2UuXr/ZqcMF8h8JERERKQpBhYXNMzDwkdCRERE2mBgaYEQAvUNLGxhISIi0ggDSwsaHgcBHNZMRESkFd6BW2C7JLDo9WxhISIi0gIDSwucW1gYWIiIiLTAwNKCS1tYdBIDCxERkRYYWFrAFhYiIiLtMbC0oCGwSBKgY2AhIiLSBANLCxpffMiwQkREpBUGlhY0TBrHOViIiIi0w8DSgsYWFlYVERGRVngXbkHDKCE2sBAREWmHgaUFSguLnlVFRESkFd6FW8A3NRMREWmPgaUFHCVERESkPQaWFtjYwkJERKQ5BpYWyPXDmtnCQkREpB0GlhbYZLawEBERaY2BpQXsdEtERKQ9BpYWyKIhsLCqiIiItMK7cAtsHCVERESkOQaWFsjsw0JERKQ5BpYWsIWFiIhIewwsLWCnWyIiIu0ZtC5AR2ern4eFgYWI6NoiyzLq6uq0LsZPntFohF6vv+rzMLC0wC7YwkJEdC0RQiA/Px+lpaVaF4XqBQQEIDw8HJLU+nspA0sLGiaOYx8WIqJrQ0NYCQ0NhZeX11XdJOnqCCFQXV2NwsJCAEBERESrz8XA0oLGPizs7kNE1NHJsqyEleDgYK2LQwA8PT0BAIWFhQgNDW314yHehVvAUUJERNeOhj4rXl5eGpeELtXw/biaPkUMLC1QWlj0DCxERNcKPgbqWNri+8HA0oKGFhY9f/iJiIg0w8DSArl+WDMfCREREWmHgaUFsiOvcFgzERGRhhhYWqC0sLAPCxERkWYYWFpg49T8RETkBlu3bsVtt92GgIAABAcH47777sP333+vbP/hhx8wfvx4BAUFwdvbGwkJCdi7d6+y/YMPPsCgQYPg4eGBkJAQjB49WovLaDech6UFsjKsmdmOiOhaJIRATZ3s9s/1NOpVjY6pqqpCamoq+vXrh8rKSixYsACjR4/G/v37UV1djdtvvx1RUVHYtGkTwsPDsW/fPtjrnwJs3rwZo0ePxnPPPYd///vfsFqt2LJlS3tdmiYYWFrQ0MKi4yghIqJrUk2djN4LPnb75x56YRi8TK7fZh988EGnr1etWoVOnTrh0KFD+Oqrr1BUVISvv/4aQUFBAIDu3bsr+y5atAjjxo3D888/r6zr37//VV5Bx8JmgxYoLSzsw0JERO3o2LFjGD9+PG644Qb4+fkhJiYGAJCXl4f9+/dj4MCBSlj5sf3792Po0KFuLK37sYWlBTL7sBARXdM8jXocemGYJp+rxogRI9C1a1esXLkSkZGRsNvt6Nu3L6xWqzK9/WU/q4Xt14NWtbAsX74cMTEx8PDwQFJSErKysq64f2lpKWbOnImIiAiYzWbceOONl322tmTJEkiShCeffLI1RWtzMqfmJyK6pkmSBC+Twe2Lmv4rFy5cQG5uLn7/+99j6NCh6NWrFy5evKhs79evH/bv34+SkpJmj+/Xrx8yMzOvuq46MtWBZd26dUhNTcXChQuxb98+9O/fH8OGDVPexPhjVqsVd911F06dOoUNGzYgNzcXK1euRFRUVJN9v/76a/zjH/9Av3791F9JO7HVd2hiCwsREbWXwMBABAcHY8WKFTh+/Dh27NiB1NRUZfv48eMRHh6OUaNG4csvv8SJEyfwzjvvYPfu3QCAhQsX4q233sLChQtx+PBhfPfdd3j55Ze1upx2oTqwLF26FNOmTcOUKVPQu3dvpKenw8vLC6tWrWp2/1WrVqGkpATvvfceBg8ejJiYGNx+++1NOgNVVlZiwoQJWLlyJQIDA1t3Ne2ALSxERNTedDod1q5di+zsbPTt2xdPPfUUXnnlFWW7yWTCtm3bEBoainvvvRdxcXFYsmSJ8ubjIUOGYP369di0aRMGDBiAO++8s8WnH9caVX1YrFYrsrOzMW/ePGWdTqdDSkqKkvJ+bNOmTUhOTsbMmTPx/vvvo1OnTnj44YcxZ84cp1dMz5w5E8OHD0dKSgpeeumlFstisVhgsViUr8vLy9Vcistscv0oIQYWIiJqRykpKTh06JDTOiGE8veuXbtiw4YNlz3+gQcewAMPPNBu5dOaqsBSXFwMWZYRFhbmtD4sLAxHjhxp9pgTJ05gx44dmDBhArZs2YLjx49jxowZqKurw8KFCwEAa9euxb59+/D111+7XJa0tDSn4VvthS0sRERE2mv3Yc12ux2hoaFYsWIF4uPjMXbsWDz33HNIT08HAJw5cwazZ8/GmjVr4OHh4fJ5582bh7KyMmU5c+ZMu5RfFg2jhDgCnIiISCuqWlhCQkKg1+tRUFDgtL6goADh4eHNHhMREQGj0ej0+KdXr17Iz89XHjEVFhbi5ptvVrbLsozPP/8cr732GiwWi9OxDcxmM8xms5rit4qNLSxERESaU9VsYDKZEB8f7zR0ym63IzMzE8nJyc0eM3jwYBw/flyZPhgAjh49ioiICJhMJgwdOhTfffcd9u/frywJCQmYMGEC9u/f32xYcSdZ5jwsREREWlM9cVxqaiomT56MhIQEJCYmYtmyZaiqqsKUKVMAAJMmTUJUVBTS0tIAANOnT8drr72G2bNn4/HHH8exY8ewePFiPPHEEwAAX19f9O3b1+kzvL29ERwc3GS9FtjCQkREpD3VgWXs2LEoKirCggULkJ+fjwEDBmDr1q1KR9y8vDzoLunvER0djY8//hhPPfUU+vXrh6ioKMyePRtz5sxpu6toRzLnYSEiItJcq6bmnzVrFmbNmtXstk8//bTJuuTkZOzZs8fl8zd3Dq3YODU/ERGR5jj0pQV2wcBCRESkNQaWFjRMHGfgsGYiIiLN8C7cAr6tmYiIrgUxMTFYtmyZ1sVoNwwsLeAoISIiIu0xsLRAaWHRM7AQERFphYGlBcooIYmBhYiI2seKFSsQGRnpNMkqAIwcORK//OUv8f3332PkyJEICwuDj48PBg0ahE8++aTVn7d06VLExcXB29sb0dHRmDFjBiorK532+fLLLzFkyBB4eXkhMDAQw4YNw8WLFwE4Jo394x//iO7du8NsNqNLly5YtGhRq8vjCgaWFjTMw8JHQkRE1yghAGuV+5dL3rTckjFjxuDChQvYuXOnsq6kpARbt27FhAkTUFlZiXvvvReZmZn49ttvcffdd2PEiBHIy8trVZXodDr87W9/w8GDB7F69Wrs2LEDzz77rLJ9//79GDp0KHr37o3du3dj165dGDFiBGRZBuB4n9+SJUswf/58HDp0CG+++WaTFyO3tVbNw/JTwk63RETXuLpqYHGk+z/3d+cAk7dLuwYGBuKee+7Bm2++iaFDhwIANmzYgJCQENxxxx3Q6XTo37+/sv+LL76Id999F5s2bbrsvGhX8uSTTyp/j4mJwUsvvYTHHnsMf//73wEAf/zjH5GQkKB8DQB9+vQBAFRUVOCvf/0rXnvtNUyePBkA0K1bN9x2222qy6EGW1ha0BBYDOzDQkRE7WjChAl45513YLFYAABr1qzBuHHjoNPpUFlZid/+9rfo1asXAgIC4OPjg8OHD7e6heWTTz7B0KFDERUVBV9fXzzyyCO4cOECqqurATS2sDTn8OHDsFgsl93eXtjC0oLGmW6Z7YiIrklGL0drhxafq8KIESMghMDmzZsxaNAgfPHFF/jLX/4CAPjtb3+L7du3409/+hO6d+8OT09PPPTQQ7BaraqLderUKdx3332YPn06Fi1ahKCgIOzatQtTp06F1WqFl5cXPD09L3v8lba1JwaWFsgc1kxEdG2TJJcfzWjJw8MDDzzwANasWYPjx4+jZ8+euPnmmwE4OsA++uijGD16NACgsrISp06datXnZGdnw263489//rPy7r+3337baZ9+/fohMzMTzz//fJPje/ToAU9PT2RmZuJXv/pVq8rQGgwsLWhoYdFxlBAREbWzCRMm4L777sPBgwcxceJEZX2PHj2wceNGjBgxApIkYf78+U1GFLmqe/fuqKurw6uvvooRI0bgyy+/RHp6utM+8+bNQ1xcHGbMmIHHHnsMJpMJO3fuxJgxYxASEoI5c+bg2WefhclkwuDBg1FUVISDBw9i6tSpV3X9V8LnHC2YMjgGM4Z0Q4ivSeuiEBHRde7OO+9EUFAQcnNz8fDDDyvrly5disDAQNx6660YMWIEhg0bprS+qNW/f38sXboUL7/8Mvr27Ys1a9YgLS3NaZ8bb7wR27Ztw4EDB5CYmIjk5GS8//77MBgc7Rzz58/H008/jQULFqBXr14YO3YsCgsLW3/hLpCEUDHuqgMrLy+Hv78/ysrK4Ofnp3VxiIhIA7W1tTh58iRiY2Ph4eGhdXGo3pW+L67ev9nCQkRERB0eAwsREdF1ZM2aNfDx8Wl2aZhL5VrETrdERETXkfvvvx9JSUnNbjMajW4uTdthYCEiIrqO+Pr6wtfXV+titDk+EiIiIqIOj4GFiIiuO62do4TaR1t8P/hIiIiIrhsmkwk6nQ7nzp1Dp06dYDKZIHHiT80IIWC1WlFUVASdTgeTqfVzmjGwEBHRdUOn0yE2Nhbnz5/HuXMavD+ImuXl5YUuXboorwJoDQYWIiK6rphMJnTp0gU2mw2yLGtdnJ88vV4Pg8Fw1S1dDCxERHTdkSQJRqPxmh7GS87Y6ZaIiIg6PAYWIiIi6vAYWIiIiKjDu276sDS8dLq8vFzjkhAREZGrGu7bDffxy7luAktFRQUAIDo6WuOSEBERkVoVFRXw9/e/7HZJtBRprhF2ux3nzp2Dr69vm04SVF5ejujoaJw5cwZ+fn5tdl5qinXtPqxr92Fduxfr233aqq6FEKioqEBkZOQV52m5blpYdDodOnfu3G7n9/Pz4w+/m7Cu3Yd17T6sa/difbtPW9T1lVpWGrDTLREREXV4DCxERETU4TGwtMBsNmPhwoUwm81aF+W6x7p2H9a1+7Cu3Yv17T7uruvrptMtERERXb/YwkJEREQdHgMLERERdXgMLERERNThMbAQERFRh8fA0oLly5cjJiYGHh4eSEpKQlZWltZFuqalpaVh0KBB8PX1RWhoKEaNGoXc3FynfWprazFz5kwEBwfDx8cHDz74IAoKCjQq8fVjyZIlkCQJTz75pLKOdd22zp49i4kTJyI4OBienp6Ii4vDN998o2wXQmDBggWIiIiAp6cnUlJScOzYMQ1LfG2SZRnz589HbGwsPD090a1bN7z44otO76JhXbfO559/jhEjRiAyMhKSJOG9995z2u5KvZaUlGDChAnw8/NDQEAApk6disrKyqsvnKDLWrt2rTCZTGLVqlXi4MGDYtq0aSIgIEAUFBRoXbRr1rBhw8Qbb7whcnJyxP79+8W9994runTpIiorK5V9HnvsMREdHS0yMzPFN998I2655RZx6623aljqa19WVpaIiYkR/fr1E7Nnz1bWs67bTklJiejatat49NFHxd69e8WJEyfExx9/LI4fP67ss2TJEuHv7y/ee+89ceDAAXH//feL2NhYUVNTo2HJrz2LFi0SwcHB4sMPPxQnT54U69evFz4+PuKvf/2rsg/runW2bNkinnvuObFx40YBQLz77rtO212p17vvvlv0799f7NmzR3zxxReie/fuYvz48VddNgaWK0hMTBQzZ85UvpZlWURGRoq0tDQNS3V9KSwsFADEZ599JoQQorS0VBiNRrF+/Xpln8OHDwsAYvfu3VoV85pWUVEhevToIbZv3y5uv/12JbCwrtvWnDlzxG233XbZ7Xa7XYSHh4tXXnlFWVdaWirMZrN466233FHE68bw4cPFL3/5S6d1DzzwgJgwYYIQgnXdVn4cWFyp10OHDgkA4uuvv1b2+eijj4QkSeLs2bNXVR4+EroMq9WK7OxspKSkKOt0Oh1SUlKwe/duDUt2fSkrKwMABAUFAQCys7NRV1fnVO833XQTunTpwnpvpZkzZ2L48OFOdQqwrtvapk2bkJCQgDFjxiA0NBQDBw7EypUrle0nT55Efn6+U337+/sjKSmJ9a3SrbfeiszMTBw9ehQAcODAAezatQv33HMPANZ1e3GlXnfv3o2AgAAkJCQo+6SkpECn02Hv3r1X9fnXzcsP21pxcTFkWUZYWJjT+rCwMBw5ckSjUl1f7HY7nnzySQwePBh9+/YFAOTn58NkMiEgIMBp37CwMOTn52tQymvb2rVrsW/fPnz99ddNtrGu29aJEyfw+uuvIzU1Fb/73e/w9ddf44knnoDJZMLkyZOVOm3u3xTWtzpz585FeXk5brrpJuj1esiyjEWLFmHChAkAwLpuJ67Ua35+PkJDQ522GwwGBAUFXXXdM7CQZmbOnImcnBzs2rVL66Jcl86cOYPZs2dj+/bt8PDw0Lo41z273Y6EhAQsXrwYADBw4EDk5OQgPT0dkydP1rh015e3334ba9aswZtvvok+ffpg//79ePLJJxEZGcm6vo7xkdBlhISEQK/XNxkxUVBQgPDwcI1Kdf2YNWsWPvzwQ+zcuROdO3dW1oeHh8NqtaK0tNRpf9a7etnZ2SgsLMTNN98Mg8EAg8GAzz77DH/7299gMBgQFhbGum5DERER6N27t9O6Xr16IS8vDwCUOuW/KVfvmWeewdy5czFu3DjExcXhkUcewVNPPYW0tDQArOv24kq9hoeHo7Cw0Gm7zWZDSUnJVdc9A8tlmEwmxMfHIzMzU1lnt9uRmZmJ5ORkDUt2bRNCYNasWXj33XexY8cOxMbGOm2Pj4+H0Wh0qvfc3Fzk5eWx3lUaOnQovvvuO+zfv19ZEhISMGHCBOXvrOu2M3jw4CZD9I8ePYquXbsCAGJjYxEeHu5U3+Xl5di7dy/rW6Xq6mrodM63L71eD7vdDoB13V5cqdfk5GSUlpYiOztb2WfHjh2w2+1ISkq6ugJcVZfd69zatWuF2WwW//rXv8ShQ4fEr3/9axEQECDy8/O1Lto1a/r06cLf3198+umn4vz588pSXV2t7PPYY4+JLl26iB07dohvvvlGJCcni+TkZA1Lff24dJSQEKzrtpSVlSUMBoNYtGiROHbsmFizZo3w8vIS//3vf5V9lixZIgICAsT7778v/ve//4mRI0dyqG0rTJ48WURFRSnDmjdu3ChCQkLEs88+q+zDum6diooK8e2334pvv/1WABBLly4V3377rTh9+rQQwrV6vfvuu8XAgQPF3r17xa5du0SPHj04rNkdXn31VdGlSxdhMplEYmKi2LNnj9ZFuqYBaHZ54403lH1qamrEjBkzRGBgoPDy8hKjR48W58+f167Q15EfBxbWddv64IMPRN++fYXZbBY33XSTWLFihdN2u90u5s+fL8LCwoTZbBZDhw4Vubm5GpX22lVeXi5mz54tunTpIjw8PMQNN9wgnnvuOWGxWJR9WNets3Pnzmb/jZ48ebIQwrV6vXDhghg/frzw8fERfn5+YsqUKaKiouKqyyYJccnUgEREREQdEPuwEBERUYfHwEJEREQdHgMLERERdXgMLERERNThMbAQERFRh8fAQkRERB0eAwsRERF1eAwsRERE1OExsBAREVGHx8BCREREHR4DCxEREXV4DCxERETU4f0/QnxasT20BxAAAAAASUVORK5CYII=\n","text/plain":["\u003cFigure size 640x480 with 1 Axes\u003e"]},"metadata":{},"output_type":"display_data"}],"source":["# accuracies\n","plt.plot(r.history['accuracy'], label='acc')\n","plt.plot(r.history['val_accuracy'], label='val_acc')\n","plt.legend()\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"JI7if0od9OhO"},"source":["### Modifying the model for Predictions\n","While training, we know the actual inputs to the decoder for all the output words in the sequence. However, during predictions the next word will be predicted on the basis of the previous word, which in turn is also predicted in the previous time-step.\n","\n","While making actual predictions, the full output sequence is not available, in fact that is what we have to predict. During prediction the only start and end token is avaialbale to us. We will use this to predict the output sentences."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"olfsbpgy9OhO"},"outputs":[],"source":["encoder_model = Model(encoder_inputs_placeholder, encoder_outputs)\n","\n","# next we define a T=1 decoder model\n","encoder_outputs_as_input = Input(shape=(max_len_input, LATENT_DIM * 2,))\n","decoder_inputs_single = Input(shape=(1,))\n","decoder_inputs_single_x = decoder_embedding(decoder_inputs_single)\n","\n","# no need to loop over attention steps this time because there is only one step\n","context = one_step_attention(encoder_outputs_as_input, initial_s)\n","\n","# combine context with last word\n","decoder_lstm_input = context_last_word_concat_layer([context, decoder_inputs_single_x])\n","\n","# lstm and final dense\n","o, s, c = decoder_lstm(decoder_lstm_input, initial_state=[initial_s, initial_c])\n","decoder_outputs = decoder_dense(o)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6YFos1aj9OhO"},"outputs":[],"source":["# create the model object\n","decoder_model = Model(\n","  inputs=[\n","    decoder_inputs_single,\n","    encoder_outputs_as_input,\n","    initial_s,\n","    initial_c\n","  ],\n","  outputs=[decoder_outputs, s, c]\n",")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dZSm_kmH9OhP"},"outputs":[],"source":["decoder_model.save(\"darija_to_english_decoder_model.h5\")\n","encoder_model.save(\"darija_to_english_encoder_model.h5\")"]},{"cell_type":"markdown","metadata":{"id":"mbQ2O5kA9OhP"},"source":["### Making Predictions\n","\n","In the tokenization steps, we converted words to integers. The outputs from the decoder will also be integers. However, we want our output to be a sequence of words in the French language. To do so, we need to convert the integers back to words. We will create new dictionaries for both inputs and outputs where the keys will be the integers and the corresponding values will be the words."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LzVagrhP9OhQ"},"outputs":[],"source":["idx2word_darija = {v:k for k, v in word2idx_inputs.items()}\n","idx2word_english = {v:k for k, v in word2idx_outputs.items()}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jDw-bcKq9OhQ"},"outputs":[],"source":["import pickle\n","pickle.dump(idx2word_darija, open(\"idx2word_darija.pkl\", \"wb\" ))\n","pickle.dump(idx2word_english, open(\"idx2word_english.pkl\", \"wb\" ))\n","pickle.dump(tokenizer_inputs, open(\"tokenizer_darija.pkl\", \"wb\" ))"]},{"cell_type":"markdown","metadata":{"id":"5Zto4nY79OhQ"},"source":["The decode_sequence() method will accept an input-padded sequence french sentence (in the integer form) and will return the translated english sentence."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rTHHLzVd9OhR"},"outputs":[],"source":["def decode_sequence(input_seq):\n","  # Encode the input as state vectors.\n","  enc_out = encoder_model.predict(input_seq)\n","\n","  # Generate empty target sequence of length 1.\n","  target_seq = np.zeros((1, 1))\n","\n","  # Populate the first character of target sequence with the start character.\n","  # NOTE: tokenizer lower-cases all words\n","  target_seq[0, 0] = word2idx_outputs['\u003csos\u003e']\n","\n","  # if we get this we break\n","  eos = word2idx_outputs['\u003ceos\u003e']\n","\n","  # [s, c] will be updated in each loop iteration\n","  s = np.zeros((1, LATENT_DIM_DECODER))\n","  c = np.zeros((1, LATENT_DIM_DECODER))\n","\n","  # Create the translation\n","  output_sentence = []\n","  for _ in range(max_len_target):\n","    o, s, c = decoder_model.predict([target_seq, enc_out, s, c])\n","\n","    # Get next word\n","    idx = np.argmax(o.flatten())\n","\n","    # End sentence of EOS\n","    if eos == idx:\n","      break\n","\n","    word = ''\n","    if idx \u003e 0:\n","      word = idx2word_english[idx]  # changed from idx2word_trans to idx2word_english\n","      output_sentence.append(word)\n","\n","    # Update the decoder input\n","    # which is just the word just generated\n","    target_seq[0, 0] = idx\n","\n","  return ' '.join(output_sentence)"]},{"cell_type":"markdown","metadata":{"id":"er-dotv69OhR"},"source":["### Testing the model\n","To test the model, we use the french sentences from the test set, retrieve the corresponding padded sequence for the sentence, and will pass it to the decode_sequence() method. The method will return the translated sentence as shown below."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6ork_taT9OhS"},"outputs":[],"source":["test_actual_sentence = []\n","test_predicted_sentence = []\n","for i in range(len(darija_test)):  # replace fr_test with your actual Darija test data variable\n","    input_seq = encoder_inputs_test[i:i+1]  # assuming that encoder_inputs_test contains the preprocessed Darija test data (i.e., tokenized, padded, etc)\n","    translation = decode_sequence(input_seq)\n","    test_actual_sentence.append(english_test[i])  # assuming english_test contains the corresponding English translations\n","    test_predicted_sentence.append(translation)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_X7xbP919OhS","outputId":"4c9ae701-d77d-462a-c03f-1bd550612862"},"outputs":[{"name":"stdout","output_type":"stream","text":["-\n","Input sentence: soyez polis avec vos parents\n","Predicted translation: be polite to your parents\n","Actual translation: be polite to your parents \u003ceos\u003e\n","-\n","Input sentence: ma vie pour une bière\n","Predicted translation: my life is like to beer\n","Actual translation: im dying for a beer \u003ceos\u003e\n","-\n","Input sentence: je suis heureuse que vous veniez\n","Predicted translation: im glad youre coming\n","Actual translation: im glad youre coming \u003ceos\u003e\n","-\n","Input sentence: sa voiture a deux ans\n","Predicted translation: his car is two years old\n","Actual translation: his car is two years old \u003ceos\u003e\n","-\n","Input sentence: désolée je ne vous ai pas entendus\n","Predicted translation: sorry i didnt hear you\n","Actual translation: sorry i didnt hear you \u003ceos\u003e\n"]}],"source":["for i in np.random.randint(0, len(darija_test), 5):  # Adjust the range to the size of your test data\n","    print('-')\n","    print('Input sentence:', darija_test[i])  # Darija test sentence\n","    print('Predicted translation:', test_predicted_sentence[i])  # Predicted English translation\n","    print('Actual translation:', english_test[i])  # Actual English translation"]},{"cell_type":"markdown","metadata":{"id":"JtNeGfO19OhS"},"source":["### Evaluation metric: BLEU Score\n","\n","BLEU, or the Bilingual Evaluation Understudy, is a score for comparing a candidate translation of text to one or more reference translations. NLTK also provides a function called corpus_bleu() for calculating the BLEU score for multiple sentences such as a paragraph or a document.\n","\n","The references must be specified as a list of sentences where each sentence is a list of references and each alternative reference is a list of tokens, e.g. a list of lists of lists of tokens. The candidate sentences must be specified as a list where each sentence is a list of tokens\n","\n","Here we will be passing predicted translations as list of candidate sentences and actual sentences as list of reference sentences.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5xlpRGTA9OhS"},"outputs":[],"source":["import nltk\n","pred=[]\n","for words in test_predicted_sentence:\n","    pred.append(words.split())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N1Fpszjh9OhT"},"outputs":[],"source":["actual=[]\n","for words in test_actual_sentence:\n","    actual.append(words.split())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2ly9QNEa9OhT","outputId":"5497b39e-b2ac-4078-dbe8-26ea645781ec"},"outputs":[{"name":"stdout","output_type":"stream","text":["0.2860901377075264\n"]}],"source":["from nltk.translate.bleu_score import SmoothingFunction\n","chencherry = SmoothingFunction()\n","BLEUscore = nltk.translate.bleu_score.corpus_bleu(actual,pred,smoothing_function=chencherry.method4)\n","print(BLEUscore)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v6ng1Ukl9OhT","outputId":"795d1c67-e710-4a98-fd10-15d7c6b56768"},"outputs":[{"name":"stderr","output_type":"stream","text":["WARNING:root:Could not import signal.SIGPIPE (this is expected on Windows machines)\n"]},{"name":"stdout","output_type":"stream","text":["BLEU = 37.88 68.3/51.5/41.6/35.5 (BP = 0.794 ratio = 0.812 hyp_len = 59438 ref_len = 73164)\n"]}],"source":["import sacrebleu\n","x=sacrebleu.raw_corpus_bleu(test_predicted_sentence,[test_actual_sentence])\n","print(x)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P1RtKeCD9OhT"},"outputs":[],"source":[]}],"metadata":{"colab":{"name":"","version":""},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.9"}},"nbformat":4,"nbformat_minor":0}